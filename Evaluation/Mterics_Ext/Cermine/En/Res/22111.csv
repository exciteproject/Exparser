Page 2 of 25Quantitative Finance123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960LOCAL LIKELIHOOD ESTIMATORS IN A REGRESSIONMODEL FOR STOCK RETURNS1Uwe Christian JönckDepartment Mathematik (SPST), Universität HamburgBundesstr. 55, 20146 Hamburg, Germany30 March 2007AbstractFWe oconsider a non-stationary regression type model for stock returns in which theinnovations are described by four-parameter distributions and the parameters are as-sumed to be rsmooth, deterministic functions of time. Incorporating also normal distri-butions for modelling the innovations, our model is capable of adapting to light-tailedinnovations as wPellas to heavy-tailed ones. Thus, it turns out to be a very exibleeapproach. Both, for the tting of the model and for forecasting the distributions offuture returns, we use local likelihood methods for estimation of the parameters. Weapply our model to the S&eP500 return series, observed over a period of twelve years.We show that it ts these datraquite well and that it yields reasonable one-day-aheadforecasts.RAMS 2000 classi cations: primary: 62P20; secondary: 62G08, 91B70, 91B84evKey words and phrases: distributional forecasts, nancial time series, local likelihoodestimation, local stationarity, Monte Carlo test, non-stationary time series, volatilityiShort title: Local likelihood modelling ew1IntroductionODuring the past twenty years remarkable e orts were made to develop and investigate modelsnfor time series of stock returns, i.e. for time series Xt = log(Pt=Pt 1), t = 1; : : : ; n, wherePt represents the price of a stock (or a stock index) at time t. In doing so the ARCH andGARCH models of Engle (1982) and Bollerslev (1986) were in the centrleof interest. Thebasic GARCH(p; q) model takes the form y(1.1)Xt = t"t;with white noise f"tg and volatilities given byt2 =p0 + Xi=1qiXt2 i + X j t2 j :j=1There have been various modi cations on this approach, which led to more sophisticated buton the other hand also more complicated models. For an overview see for example Bollerslev1This work was supported by Deutsche Forschungsgemeinschaft (DFG grant DR 271/4 1).E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859602LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 3 of 25et al. (1994) or Gouriéroux (1997). These models succeed in reproducing various stylizedfacts. However, especially when dealing with long time series, they also have some draw-backs, such as the occurence of the so-called IGARCH e ect in the popular GARCH(1;1)model. (A critical discussion of this model can be found in St ric , 2003.) Mikosch andSt ric  (2003, 2004) argue that this e ect could be due to the fact that time series of logreturns are actually not stationary, which is one of the basic paradigms of the GARCH classmodels. Furthermore, they show that also the so-called long range dependence e ects in thesecond moment structure can be explained if the underlying time series is non-stationary.The assumption of stationarity might also seem questionable especially for long nancialtime series, because the nancial markets and the general economic framework may changeover long Ftime periods. Consequently, the concept of stationarity might not be adequateofor a nancial time series model. A weakening of the strong requirement of stationaritymight be the concept of a locally stationary behaviour of the time series: this means that forsmall time intervrals the time series is nearly stationary, whereas over longer time intervalsPits (stochastic) behaviour changes gradually.Drees and St ric  (2002) propose a very simple regression-based model that drops theassumption of stationarity. eFor the returns Xt Drees and St ric  presume a multiplicativeestructure, similar to that of (1.1). The di erence to the GARCH models lies in the mod-elling approach for the volatilities t. In the GARCH class models these are determined byendogenous factors, namely by parstvolatilities as well as past returns. In contrast to that,RDrees and St ric  assume the volatilities to be driven by some unknown (presumably rathercomplex) market conditions, and hence to be determined exogenously. However, this exte-rior mechanism might be too complex to deevelop a realistic stochastic model for it. For thisvreason Drees and St ric  simply model the volatilities as a smooth, deterministic functionof time. Thus, their basic model is the following:ie=const:;8> Xt = + (t)"t; t = 1; 2; : : : ; n;><> f"t : t = 1; : : : ; ng IID(0; 1); w>: (t); t = 1; 2; : : : ; n; a smooth, deterministic function of time.(1.2)Considering the centered returns Rt = Xt , we have ORt2 = (t)2 + (t)2("t2 1):nHence, (1.2) can be interpreted as a regression model for the squared volatilities, and non-lyparametric regression techniques can be applied for estimating the volatilities. The innova-tions "t are modelled by asymmetric Pearson Type VII distributions, which generalize theclass of t-distributions (see below), and thus allow for heavy tails. Drees and St ric  t thismodel to the daily returns of the closing prices of the Standard & Poor's 500 (S&P 500)stock index from 2 January 1990 to 21 February 2002, depicted in Figure 1.1. They esti-mate the volatilities by a simple Nadaraya-Watson kernel estimator and they use a maximumlikelihood (ML) estimator to determine the parameters of the t-distributions. They showthat their model is competitive to the conventional GARCH(1;1) and EGARCH(1;1) modelsconcerning the tting of the data as well as the forecasting of future return distributions.In this paper (which can be regarded as a continuation of the work of Drees and St ric ,2002) we take up and re ne the above regression model. However, instead of the assump-E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufPage 4 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS31234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859600.080.060.040.020-0.02-0.04-0.06-0.081990199219941996199820002002Figure 1.1. Daily returns of the closing prices of the S&P 500 index from 2 January 1990to 21 February 2002.Fotion of identically distributed innovations, we model the parameters of the distributions assmooth functionrsof time. Furthermore, we expand the model by modelling innovationswith very light tails by normal distributions. These two new aspects increase the model'sexibility. For both, Pthetting of this re ned model to the S&P 500 return series and theforecasting of future retuerndistributions, we use local likelihood estimators. These esti-mators combine the idea of ML estimation and nonparametric regression techniques. Wediscuss their asymptotic behaeviour, which enables us to construct asymptotic con denceintervals for the true parameters. rThe theoretical background for these asymptotics is basedRon a paper of Aerts and Claeskens (1997). Assessments of the estimated parameters and theestimated forecasts, respectively, show that our approach seems to provide a good tting ofthe data as well as reasonable forecasts. evThe paper is organized as follows: in Section 2 we present the underlying model. InSection 3 we give a brief overview of the method of local likelihood estimation, and wediscuss the construction of asymptotic con dence iintervals. In Section 4 we t the modeleto the S&P 500 data and evaluate the results with several testing devices. We also provideforecasts of future return distributions, which is done in wSection 5. Section 6 concludes.O2 Re nement of the model of Drees and St ric nlWe consider the regression model (1.2) from the previous section. In the following, instead ofthe returns Xt we consider the centered returns Rt = Xt . Then we have y(approximately)(2.1)Rt = (t)"tXtXn;regarding Xn = 1=n Pjn=1 Xj as an approximation for = EXt, and neglecting an estima-tion error that might occur. We assume the innovations "t and, consequently, the centeredreturns, too to be independent and distributed with densityg(x; u+; u ; v+; v ) = 21 g(jxj; u ; v ) fx < 0g + g(x; u+; v+) fx0g ;u 2 [0; 2);2 (0; 1);E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 5 of 254where(2.2)8>> (1=2u(11==u2))p>>g(x; u; v) = <pvu 1 + xv 2 u1=ufx0g; u > 0;:>>>> vp2 exp nx 2ovfx0g;u = 0:The corresponding cdf is denoted G( ; u+; u ; v+; v ), or sometimes just G. The parametersu+ and u essentially determine the shape of the density g: for u > 0 the term 1=2g( ; u; v)equals the right tail of the density of a t-distribution with 2=u 1 degrees of freedom andscale parameter v=(2 u)1=2 (which is sometimes also referred to as a Pearson Type VIIdistributFion, cf. Johnson and Kotz, 1970, p. 114 .), and for u = 0 it equals the right tail ofthe density of a normal distribution with mean zero and variance v2=2. The parameters v+and v behave olike scale parameters for the positive and negative parts of the distribution,respectively. It irseasily seen that G has nite variance i u+; u < 2=3. This modellingapproach is more exible than that of Drees and St ric  (2002), who always assume u+; u >0. In contrast, our mPodel can adapt to heavy-tailed as well as light-tailed innovations aswe shall see later on. eNote that for each x we have g(x; u; v) ! g(x; 0; v) as u ! 0. Hence, g(x; 0; v) canalso be considered an approxiemation to g(x; u; v) for small values u. In the following thisproperty is often referred to as threnormal approximation of the tails.Another nice property of the law G is that it allows for asymmetry in the tails, and thus isquite exible. It may appear somewhat Rinconsequent to model the innovations (and thus theecentered returns) presumed to have mean zero by a class of distributions that in generaldo not have this property (the distributions G are in general not centered, but only havemedian zero). However, for practical applicativonsthis is not of great importance: indeed,the model assessments in Section 4 will show that ithe means of the tted distributions areapproximately equal to zero. eWe further re ne the model of Drees and St ric  by dwropping their assumption of identi-cally distributed innovations. Instead, we assume the parameters to be smooth, deterministicfunctions of time, i.e. we have as an initial approachO"t G( ; u+(t); u (t); v+(t); v (t)):nTo avoid an overparametrization resulting from the additional scale parameter (t), weintroduce new parameters +(t) := (t)v+(t) and (t) := (t)v (t), wlhyichleads to themodelling assumption of independent centered returns(2.3)RtG( ; u+(t); u (t); +(t); (t)):Hence, our model is completely determined by the distributions of the centerd returns, givenby (2.3), and the smooth parameter function: f1; : : : ; ng ! [0; 2)2(0; 1)2; t 7! BBBB@ 23((tt))CCCCA0 1(t)14(t)0u+(t)1BBB@u+((tt))CCCC :BA(t)E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufPage 6 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS5In the following, we shortly write G( ; ) or G , instead of G( ; u+; u ; +; ), and for thecorresponding density we write g . Note that in this approach the volatilities only playa subordinate role as part of the parameters + and . However, assuming that for theinnovations we have Var("t) = 1, the volatilities can easily be regained from the equation2(t) = Var(Rt).3Local likelihood estimation(3.2)>>>:= ( 1T ; : : : ; dT )T = ( 11; : : : ; O1p1; : : : ; d1; : : : ; dpd )T :Here K denotes a kernel, h is a bandwidth (or smoothing parameter), anndKh( ) = K( =h)=h.Usually K is a unimodal symmetric pdf, preferably with compact support. MaximizingLn( ; h; x) with respect to yields the local likelihood estimates ^kj . Conlseyquently, we haveestimates ^k(j)(x) = j! ^kj for the value of the parameter function in x and its derivatives,respectively. For further information on local likelihood estimation and local polynomialtting cf.Aerts and Claeskens (1997), Fan et al. (1998), or Fan and Gijbels (1996).Asymptotics. In the following, we assume the design points xi to be generated accordingto xi = G 1 (i 1)=(n 1) , with G(x) = R x1 fX(t)dt and a design density fX withsupp(fX ) = [a; b]. Furthermore, we assume the kernel K to be a symmetric pdf with compactsupport [ 1; 1]. Aerts and Claeskens (1997) show that under some additional regularityconditions the local likelihood estimators are consistent and asymptotically normal. (Mainly,these regularity conditions are the classical conditions on the densities f ( ; ), needed forFor tting our model to data we use the method of local likelihood estimation. In thissection, we rst give a brief overview of its basic concept. Subsequently, based on a paperby Aerts Fand Claeskens (1997), we investigate the asymptotics of these estimators, and weprovide a deovice for constructing approximate con dence intervals for the parameters. Wediscuss how these ideas carry over to our model.rThe basic concept. PConsider a sample (x1; y1); : : : ; (xn; yn). The data xi denote distinctpoints of time in some interval [a; b]. These might either be xed or given by a randomdesign. The corresponding eyi's are realizations of independent, real-valued random variablesYi. Suppose that each Yi is diestributed according to some pdf f ( ; si), where the parametersi 2 d is determined by a functiorn: xi 7! (xi) = si. Assuming this parameter functionto be su ciently smooth, each of its components k can be approximated locally by a Taylorpolynomial of degree pk. Thus, for xi Rclose to a xed point x we have the approximation(3.1) k(xi) Xjp=k0 k(jj)!(x) (xeivx)j Xjp=k0 kj (xi x)j ;iwhere in this notation the dependence of kj from x is omitted. For estimating the coe -cients kj of the local polynomials, we consider the leocal likelihood function>8 n 0 p1>>< Ln( ; h; x) = X log f @yi; Xi=1 j=01j(xiwpdx)j ; : : : ; Xj=0dj(xi1x)j A Kh(xix);123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf6(3.3)proving consistency and asymptotic normality of ML estimators; cf. Aerts and Claeskens,1997, conditions (R1) (R6), for details.) More precisely, they show that123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 7 of 25pnh^1(x)1?(x) T Hp1; : : : ; ^d(x)d?(x) T HpdTp1nh Bnh(x)D! N (0; V(x));as n ! 1, with k?(x) = ( k?1(x); : : : ; k?pk (x))T denoting the vector of the true coe cientsof the local polynomials, i.e. k?j(x) = k(j)(x)=j!, and employing the indexing given in(3.2). The matrices Hpk are diagonal matrices with entries h0; : : : ; hpk , whereas the otherquantities depend in a rather complicated way on the true parameters k(j)(x), the kernelK, the (Flog-)densities and its derivatives and on the design point x. Again, cf. Aerts andClaeskens (1997) for details.oCon dence intrervals. The results from the previous paragraph provide a device forconstructing approximate pointwise con dence intervals for the true coe cients of the localpolynomials. With eij Pdenoting the vector with 1 as ij-entry and 0 elsewhere (i.e., weehave eiTj = ij ), from (3.3) we getpnh ^ij(x) i?ej(x) hj p1nh eiTjBnh(x) D! N (0; eiTjV(x)eij ):rFollowing the argumentation of Fan and Yao (2003), p. 243, we neglect the asymptotic biasterm. Hence, approximately, we have RL ^i0(x) i?0(x) eN 0; n1h eiT0V(x)ei0 :Replacing the parameters i(x) in the expression vfor the asymptotic covariance matrix V(x)by the corresponding estimates ^i(x) we get an appiroximate covariance matrix Vb(x). Thus,by eu =2 Vbi0;i0(x)=(nh) 1=2 ; ^i0(x) + u w=2Vb i0;i0(x)=(nh) 1=2ih ^i0(x)we get an approximate con dence interval at the level 1where u =2 is the standard normal (1 =2)-quantile.Ofor the true parameter i(x),Bandwidth selection. A crucial point in local likelihood estimatinonis the choice of anappropriate bandwidth h. This acts as a smoothing parameter in nearly the same way asit does in ordinary nonparametric regression. The above asymptotics mlotivate to choseya bandwidth that minimizes the asymptotic mean squared error of the estimates, whichis a function of h and also depends on some unknown constants. The latter ones haveto be estimated by plug-in procedures. (Cf. also Aerts and Claeskens, 1997.) Anotherapproach, which is not based on the above asymptotics, is discussed by Fan et al. (1998):they derive estimates for the bias and variance terms, and thus for the mean squared error(MSE) of the local likelihood t. Following their approach the bandwidth is chosen suchthat the estimated MSE, which is a function of h, is minimal. This method is especiallyfeasible for the computation of local bandwidths. A nice alternative to the above bandwidthselection procedures is a cross-validation method, proposed by Aerts and Claeskens (1997).In this fully data-driven approach for a given bandwidth h the parameters (x1); : : : ; (xn)E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufPage 8 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960are estimated, where the ith estimate is based on the sample without the ith oberservation(xi; yi). The resulting parameter estimates denoted ^[i](xi) are then plugged into the log-likelihood function. Doing these computations for varying bandwidths h yields the cross-validation functionnCV (h) = X log f (yi; ^[i](xi));i=1which is then maximized with respect to h. This cross-validation approach is easy to im-plement and we will also consider it later for our estimation problems in Sections 4 and5.ApplyinFgthe asymptotics to our model. For applying the above asymptotics to ourmodelling approach, we have to consider some minor modi cations to our model. Aftercentering the oreturns (cf. (2.1)), we are given observations f(t; Rt) : t = 1; : : : ; ng. For thepurpose of asymrptotic investigations, now we consider a rescaling of the time points by thetransformation(3.4) Pes 7! ns 11 =: s~; 1 s n;ewhich maps the time points into the unit interval. These rescaled design points ful llthe assumption on the points xi from the above paragraph on asymptotics with fX (x) =[0;1](x). The parameter functiorncan be rescaled in the same way, setting ~(t~) := (t).RNote that this transformation is only necessary for the application of the asymptotic resultsand the computation of the con dence intervals. It has no e ects on practical aspects ofthe estimation. Therefore, when it is obvioeusthat an assertion is correct for the originalpoints of time (i.e. for t = 1; : : : ; n) as well as for the rescaled ones (i.e. for the points t~), forvease of notation we often silently omit the notation for the rescaling. Especially, we do sowhen considering the parameter function and the liocal polynomials.eIn addition, we have to check whether the regularity conditions on the densities holdfor our model. Actually, this is the case for a suitable wsubmodel, namely for the familyfG : 2 g. Here the parameter space is de ned as := (u; u)2 ( ; )2, with arbitraryvalues 0 < u < u < 2=3 and 0 < < < 1. The veri cation of Otheregularity conditions islengthy but quite standard. It is based upon showing diverse interchangeability conditionsfor integration and di erentiation of the densities and rough estimatinoninequalities for thederivatives of the (log-)densities; it also uses the fact that is compact. The completeproof is given in Jönck (2005). Consequently, for estimations within thislsubmodel we canycalculate approximate con dence intervals for the true parameters according to Section 3.Since 7! (Var G )1=2 is di erentiable in , an application of the Delta method yieldsapproximate con dence intervals for the volatilities, too.4Modelling the S&P 500 return seriesIn this section we t our model to the returns of the closing prices of the S&P 500 returnseries from 2 January 1990 to 21 February 2002 (cf. Figure 1.1), which is a total of n = 3062observations. We discuss the numerical results and check whether they ful ll the modelassumptions.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859608LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 9 of 25Fitting the model. We estimate the (rescaled) parameter function (x) for each of thepoints x = x~1; : : : ; x~n, with x~i = (i 1)=(n 1), cf. equation (3.4). To this end, for eachsuch x the optimization problem(4.1)8> max Ln( ; h; x) under constraints: ( 10(x); : : : ; 40(x))T 2 ;>>>>:>><> @0Xjp=10 1j(x~i x)j ; : : : ; Xjp=40 4j(x~i x)j A1T 2 ; x~i 2 [xh; x + h];is solved, where Ln( ; h; x) is the local likelihood function de ned by (3.2), employing thedensities fg : 2 g. For the kernel K we use the Epanechnikov kernel, which is givenby K(u) F=3=4(1 u2)+. Using the notation from (3.2), the vector ( 10(x); : : : ; 40(x))Tcorresponds oto(x) = (u+(x); u (x); +(x); (x))T . For the computations we use thefmincon routine implemented in Matlab (V.7). The meaning of the rst constraint in (4.1)is obvious, the sercond one ensures that all evaluations of the local polynomials lie in thepwaitrhamue=ter10spa2,ceu, =oth2P=er3w,ise=L1n0( 4; ha;nxd) is=n5o.t de ned. We take as de ned in Section 3,We compute the cross-evalidation function CV ( ) for bandwidths ranging from 0:010 to0:075 (cf. Section 3): for bandweidths between 0:020 and 0:050 it is very at, and it has a (notvery distinct) maximum at hCV = 0:026. For our computations we choose the bandwidthh = 0:030. rWe use local linear ts for all four Rparamters, i.e. p1 = = p4 = 1. It is preferable touse the same degree of adaption for all parameters, since the expression for the asymptoticbias is essentially of an order of the smallest esuch polynomial degree. In addition, the use ofvodd-degree polynomial ts reduces boundary e ects. (For details see Aerts and Claeskens,1997.) Our choice of local linear ts takes account of both aspects and at the same timekeeps the number of optimization parameters to a iminimum.eIn addition, we use the normal approximation from Section 2 in the following way: if forsome x the local likelihood estimation yields an estimate wu^+(x) = u, we can assume the truevalue u+(x) to be very close to zero, thus implying a very light right tail of the correspondingreturn distribution L (Rt). According to the above ideas, then Owe can approximate theright tail of the return distribution by that of a N (0; +2 =2)-distribution, and thus u^+(x)is assigned the value zero. Likewise we proceed if u^ (x) = u. In canseof either u^+(x) oru^ (x) being zero, the conditions for the asymptotic results by Aerts and Claeskens (1997)are violated. Therefore, in such cases we do not provide asymptotic con dleynceintervals forany of the four paramters u+(x), u (x), +(x) and (x).Numerical results. The estimates for the parameter functions are shown in Figure 4.1.Apparently, the normal tail approximation is widely employed, and so it seems a valuabletool to enhance the model's ability of assessing light-tailed distributions.One can clearly see that for most points of time we have u^+ < u^ , i.e. the left tails of theestimated distributions are heavier than the corresponding right tails. This might re ect thewell-known fact that in time series of stock returns the extremely large negative returns areusually larger in absolute value than the extremely large positive returns. Furthermore, theestimated distributions show some sort of symmetric behaviour in the tails. For example,E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf0.90.80.70.60.50.40.30.20.10 1990 1992 1994 1996 1998 2000 2002u^+0.90.80.70.60.50.40.30.20.10 1990 1992 1994 1996 1998 2000 2002u^0.030.0250.020.0150.010.005Page 10 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS91234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859600.030.02F50.020.0150.010.005orP0 1990 1992 1994 1996 1998 2000 2002 0 1990 1992 1994 1996 1998 2000 2002^+e ^Figure 4.1. The estimated paerameter functions u^+, u^ , ^+ and ^ (bold solid lines) togetherrwith the corresponding asymptotic pointwise 95%-con dence intervals (dotted lines), whereapplicable, and the bootstrap con dence intervals (thin solid lines).Rwhenever the left tails of the distributions eget heavier, i.e. u^ increases, the parametervfunction u^+ also tends to increase and vice versa. Apparently, such a symmetric behaviourcan be observed in the estimated scale parameters ^+ and ^ , too, where this symmetryseems even much more distinctive. Remember thiat we de ned (t) = (t)v (t). Thus,edue to the symmetric behaviour of the scale parameters one might try to simplify themodel in such a way that only the volatility (t) is prewsumed a smooth function and theparameters v+(t) and v (t) are presumed global constants. However, it turns out that theOquotient ^+(t)=^ (t) is strongly uctuating with varying t and that the con dence intervalsfor +(t)= (t) (which can be derived easily from the use of the Delta method) are nearlydisjoint for many points of time. nThe estimates u^+ and u^ also seem to exhibit a certain kind of periodicity, which ex-presses through the fact that the periods between two consecutive points lof ymaximum (andminimum, respectively) are approximately of the same length. This e ect might be due tothe choice of the rather small bandwidth h for the estimation of the parameters at any pointof time less than 185 observations are taken into account. Indeed, for larger bandwidths thisperiodicity appears much less distinct.Figure 4.1 also shows the pointwise con dence intervals for the true parameters, asderived in Section 3 (depicted in dotted lines). Since we can only provide such asymptoticcon dence intervals for those points of time for which no use of the normal approximationis made for either tail (see above), we also compute 95%-bootstrap con dence intervals foreach of the estimated parameters (depicted in thin solid lines in Figure 4.1). To this end,we simulate a total of J = 1000 time series samples fRt j : t = 1; : : : ; ng, with independentE-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596010LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 11 of 250.30.250.20.150.10.050 1990 1992 1994 1996 1998 2000 20020.30.250.20.150.10.05 1990 1992 1994 1996 1998 2000 2002Figure 4.2. Left: The annualized volatilities ^a (bold solid lines) with the correspondingasymptotic pointwise 95%-con dence intervals (dotted lines), where applicable, and the boot-strap con Fdence intervals (thin solid lines). Right: The estimated annualized volatilitiesfrom our local likelihood estimation approach (solid lines) compared to the estimates fromDrees and Sto ric  (2002), which are based on a kernel estimate (dotted lines).rvariables P(4.2) Rt ejG^(t); t = 1; : : : ; n; j = 1; : : : ; J:eFor each of these simulated samples we compute likelihood estimates f ^ j (t) : t = 1; : : : ; ng.Thus, for every t and every k (denoting one of the parameter components) the upper andlower 2.5%-quantiles of the set f ^kj r(t) : j = 1; : : : ; J g yield a 95%-boostrap con dence inter-val for the parameter ^k(t). This is a very Rsimple procedure for the construction of bootstrapcon dence intervals. However, the resulting intervals should be interpreted carefully, becausewe do not know anything about their (asymeptotic) properties and thus their accuracy. Invaddition, we do not know the e ects on these intervals which occur due to the fact that forparameter values u+ or u close to zero our model switches from t-distributions to normaltails. Nevertheless, these bootstrap con dence inteirvals are helpful to get a rst impressioneon how the local likelihood estimates in our modelling approach behave.For u+ and u the asymptotic con dence intervals (as wwell as the bootstrap con denceintervals) are rather wide, in some cases they cover nearly all of the space (u; u). Thisis somewhat unsatisfying, because it yields little information Oabout the accuracy of ourestimated functions for the parameters u+ and u : these con dence intervals would as wellallow for parameter functions u+ and u of a di erent shape, for examnple they would allowfor globally constant parameter functions. It might therefore be worth thinking about asimpli ed approach, modelling u+ and u as global constants, like Drees alnd ySt ric  (2002)do. We shall come back to and further discuss this idea at the end of this section. Forthe scale parameters + and the asymptotic con dence intervals as well as the bootstrapcon dence intervals are rather narrow. Indeed, they seem to coincide more ore less.Figure 4.2 shows the estimated annualized volatilities ^a(t) := p250^(t), where the ^'sare computed as the standard deviations of the estimated distributions. The correspondingcon dence intervals for the estimated volatilities result from the use of the Delta method.The estimated annualized volatilities resemble very much the corresponding estimates fromDrees and St ric , concerning both, the oscillation behaviour and the magnitude of theestimates, as can be seen from the right image in Figure 4.2. Only the double-peak in themiddle of 1995 appears a bit strange, in particular, since the time series is very calm duringE-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufPage 12 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS11123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960that period. The occurence of such e ects depends of course on the choice of the bandwidth:they can be avoided by chosing a larger bandwidth, which on the other hand might lead toan over-smoothing of some parts of the curve. To completely prevent such problems, the useof a local, data-driven bandwidth (as proposed by Fan et al., 1998, Section 6) may be helpful,which is however computationally very intensive and therefore not always recommended.Assessment of the model. Summarizing so far, one can say that the local likelihoodestimators in our model, together with the normal tail approximation seem to yield rea-sonable results for describing the development of the time series. Next we want to checkwhether the model assumptions are really ful lled. Of course, this also gives some answerto the qFuestion of the goodness of our modelling approach. We have to check whether the (ceontered) returns are independent, the estimatred distributions G^(t) are good approximations to the real distributions ofthe returns, i.e. approximately we have Rt G^(t), andP the estimated distributions G^(t) have approximately mean zero (i.e. the centeredreturns Rt are really ecentered).For this purpose we mainly uesethe same techniques as Drees and St ric  (2002). Further-more, we check whether r the asymptotic con dence intervRalscover the true parameter with probability 0.95.eFirst we check whether the innovations or, equivalent, whether the centered returnsRt are independent. This is one of the basic assumptions of our model. As the commontests for independence always demand for a savmple of identically distributed observations,we rst standardize the centered returns. Assumiing the estimated distributions are goodapproximations for the return distributions, i.e. we ehave approximately Rt G^(t), thevariables w(4.3) Vt := 1 G(Rt; ^(t)) ; t = 1; : : : ; n;Oin the following referred to as standardized returns should be standard normal, and thusin case of independence of the returns should be Gaussian white nonise.To check this, weinvestigate the SACFs of the time series of standardized returns and of their absolute val-ues; see Figure 4.3. For almost all lags both of the SACFs stay within thle y95%-con denceintervals, given by the dashed lines, thus strongly supporting the independence assumption.In addition, Figure 4.3 shows the p-values of the Portmanteau tests for the correspondingtime series for the rst 100 lags. While for the standardized returns the hypothesis of inde-pendence is rejected for almost all lags at the 95%-level, the test supports the hypothesis ofuncorrelated absolute standardized returns for almost all lags. (For general information onthe con dence intervals or the Portmanteau test cf. Brockwell and Davis, 1987, Section 7.2and p. 300 ., respectively.) This surprising e ect occurs in a similar form also in the originalmodel of Drees and St ric  (2002). To nd an explanation for this e ect, we consider theSACFs of the returns and the absolute returns of the S&P 500, respectively, which are givenin Figure 4.4: the SACF of the absolute returns exhibits some long range dependencies (Fig-E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf0.080.060.040.02SFCA-0.020-0.04-0.06-0.08 0 10 20 30 40 L5a0g 60 70 80 90 1000.080.060.F020.04SFCA-0.020 o-0.04-0.06 r-0.08 0 10 20 30 40 L5a0g 60 70 80 90 100P10.90.80.7ev0.6lau0.5-p0.40.30.20.100 10 20 30 40 L5a0g 60 70 80 90 10010.90.80.7ev0.6lau0.5-p0.40.30.20.100 10 20 30 40 L5a0g 60 70 80 90 10012345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596012LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 13 of 25Figure 4.3. The SACFs (eleft) and the p-values of the Portmanteau tests (right) for the rst100 lags of the standardized reeturns fVt : t = 1; : : : ; ng (top) and for the absolute values ofthe standardized returns fjVtj : t = 1; : : : ; ng (bottom). The dashed lines in the plots on therleft represent the 95%-con dence intervals under the hypothesis of independence.Reure 4.4, right), which, according to Drees and St ric , might re ect certain non-stationaritieswithin the second moment structure of the time series. They argue that the locally station-ary modelling approach describes these non-svtationarities quite well. Consequently, theabsolute standardized returns behave nearly like uincorrelated random variables, as can beseen by the evaluations of the corresponding SACF aendthe Portmanteau test (Figure 4.3,bottom). However, this does not necessarily hold true for wthe standardized returns them-selves. Actually, there also might be some non-stationarities (for instance in the mean ofthe time series), which mainly manifest through the SACF of the returns, and which arenot taken into account by our modelling approach. Because the Ostandardization (4.3) itselfnonly changes the absolute values of the returns, but not their signs, the SACFs of both,the returns (Figure 4.4, left) and the standardized returns (Figure 4.3, top left) do not dif-fer very much, and thus our modelling approach cannot capture these e lects, which mightbe a reason for the negative outcome of the corresponding Portmanteau tyest. Neglectingthis aw, we conclude that the standardized returns (and thus the returns) can at leastapproximately be regarded as independent, in accordance to our model assumptions.Next we check whether the estimated distributions G^(t) are good approximations forthe true distributions of the centered returns. That is, we test for the null hypothesisRtG^(t); t = 1; : : : ; n:If this holds true, then the standardized returns should be approximately standard normal.This is clearly supported by the QQ-plot for the standardized returns, which is given inFigure 4.5. To formally test the normality of the standardized returns, we employ theE-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf0.060.040.02FCA 0S-0.02-0.04ForP43litse 2naqu 1snrteu 0rderaadn-2ize-1deS-3t0.250.2F0.15CSA0.10.05-0.06 0 10 20 30 40 L5a0g 60 70 80 90 1000 050100 Lag 150200250Figure 4.4. The SACFs of the S&P 500 returns (left) and of their absolute values (right)for the rst 100 and 250 lags, respectively.Page 14 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS13123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960--44r-3 -2Stand-a1rd nor0mal qu1antiles234Figure 4.5. QQ-Plot of the standardRizedreturns fVt : t = 1; : : : ; ng.eKolmogorov-Smirnov (KS) test and the Jarque-Bera (JB) and Shapiro-Wilk (SW) tests. Thelatter ones are specially designed for testing nvormality; cf. Shapiro and Wilk (1965), andJudge et al. (1988), respectively. (The Matlab impleimentations used for the SW and JB testsare part of the UCSD GARCH Toolbox, available at ehttp://econ.ucsd.edu/ ksheppar;we corrected some errors in the codes.) The high p-valwuesof all three tests, given in therst line of Table 4.1, indicate that the estimated distributions approximate the true returndistributions very well.Finally, we address to the question if the estimated distribuOtions G^(t) (approximately)have mean zero, as postulated by the model. If so, according to the central limit theorem,the statistic nSn := (PtnP=1tn=^12(Rt)t)1=2 lyshould be approximately standard normal. The p-value of the corresponding two-sidedtest is roughly equal to 1, and thus we accept the estimated distributions G^(t) as beingTestppMCKolmogorov-Smirnov0.8970.794Jarque-Bera0.3360.824Shapiro-Wilk0.2650.756Table 4.1. The p-values and Monte Carlo p-values for the KS, JB and SW tests for nor-mality of the standardized returns.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596014LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 15 of 25aproximately centered.Our approach of evaluating the estimated model is not unproblematic, as it has twomajor shortcomings: when testing for independence of the returns, we assume the estimateddistributions to be good approximations for the true return distributions. Whereas thegoodness of t tests used for checking normality of the standardized returns presume thereturns to be independent. We adopt this approach, because (to the best of our knowledge)there is no reasonable method for testing independence and the goodness of t separately.Consequently, the outcomes of the above evaluations should be interpreted carefully.The second shortcoming of our approach concerns the p-values of the above goodness oft tests. What we did above, was computing the estimates ^(t) on the basis of the centeredreturns aFndthen applying the tests for normality on the standardized returns. These dependon the estimates ^(t) and thus, again, on the centered returns (cf. (4.3)). So, actually, we useothe same data for estimating the model as we use for checking their goodness of t. Hence,the accuracy of trhe p-values seems questionable. Due to the high dependency on only onePsingle set of data, they may not be very accurate and there might be some need for adjustingthem. Therefor let us remind how the p-values are calculated for the case of the KS, JB andSW tests: each of these teestsis based on a test statistic, say T . It will reject the hypothesisthen de ned as p = P fT tg. eThus, we can redress the problem of an inaccurate p-value byof standard normal Vt's if the realization of T , denoted t, takes large values. The p-value isrecalculating the law of T in considreration of the standardization (4.3). To this end, supposewe are given J time series samples fRt j : t = 1; : : : ; ng, simulated according to (4.2). ForReach of these simulated samples we compute the likelihood estimates f^ j(t) : t = 1; : : : ; ngand the resulting standardized returns, given eby Vt j = 1 G(Rt j; ^ j (t)) . Consequently,we have J + 1 realizations of T given byvt = T (V1; : : : ; Vn) and tj =iT (V1 j; : : : ; Vn j):eThen, from the empirical cdf FJ ( ) = J 1 PjJ=1 ftj g we get an approximation forthe law of T under the assumption that the distributions wofthe centered returns are reallygiven by the family fG^(t) : t = 1; : : : ; ng. Consequently, for the p-value we have as anapproximation the Monte Carlo p-value pMC = 1 FJ (t). For mOoregeneral information onthis Monte Carlo testing approach cf. Davison and Hinkley (2003), especially Chapter 4. (Ofcourse, for the same reasons that we already mentioned in the discussinonof the constructionof bootstrap con dence intervals, such a Monte Carlo p-value has to be considered carefullyrst of all it is a heuristic approach to assess the quality of the model t.l)yWe do this computationally intensive method for the KS, JB and SW tests with J = 1000simulated times series. The results are given in the second line of Table 4.1. All three pMC-values assert the good results from the simple tests, the Monte Carlo version of the JBand SW tests, surprisingly, having even a higher p-value than their simple versions.Finally we want to check whether the approximate con dence intervals actually coverthe true parameter with probability 0.95. Suppose that at time t neither u^+(t) nor u^ (t)equals 0 or 2=3, such that we can construct con dence intervals for the true parameters,according to Section 3. Again, we consider the simulated time series fRt j : t = 1; : : : ; ngand the resulting local likelihood estimates f^ j(t) : t = 1; : : : ; ng, j = 1; : : : ; J . Assumethat from the estimates ^ j(t) (i.e. the parameter estimates for time t resulting from the jthE-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859601.0510.950.90.850.80.750.70.651.050.9F510.85 o0.90.75 r0.80.70.65P1990 1992 1994 1996 1998 2000 2002^+ee10.9 r0.80.70.60.50.4Re1.0510.950.90.850.80.750.70.651.0510.950.90.850.80.750.70.651990 1992 1994 1996 1998 2000 2002^Page 16 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS151990 1992 1994 1996 1998 2000 2002u^+1990 1992 1994 1996 1998 2000 2002u^1990 1992 1994 199v61998 2000 2002^a ieFigure 4.6. Relative frequencies of the initial paramter estimates ^k(t), k = 1; : : : ; 4, to liein the simulated con dence intervals. Points of time t, wforwhich in the orignial estimatesno con dence intervals are constructed, are assigned the value zero.Osimulation) we can compute approximate con dence intervals for each nofthe four parameters^1(t); : : : ; ^4(t). These may be denoted by I ^kj (t) , k = 1; : : : ; 4. The number of all j's forwhich the construction of such con dence intervals is possible be denotedlby Jt. Then theyrelative frequencies # j : ^k(t) 2 I ^kj (t) =Jt give an estimate for the covering probabilityof the approximate con dence intervals.We compute these relative frequencies for all t with u^+(t); u^ (t) 2= f0; 2=3g, using a totalof J = 1000 simulated time series. The results are depicted in Figure 4.6: for the parametersu+, u , + andthe levels of the con dence intervals lie between 0.85 and 1.0 for almostall points of time and thus do not di er too much from the 95%-level. However, there arealso some exceptions from this, e.g. shortly after the beginning of the year 2000, where theestimated level for the con dence intervals of the parameter + are very low (below 70%).A simple explanation for this e ect might be, that this is exactly the period during whichthe estimated parameters ^+ are maximal, as can be seen in Figure 4.1. The estimates ^+j ,E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596016LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 17 of 25TestppMCKolmogorov-Smirnov Jarque-Bera0.790 0.8120.787 0.834Shapiro-Wilk0.8610.879Table 4.2. The p-values and Monte Carlo p-values for the KS, JB and SW tests for nor-mality of the standardized returns when u+ and u are modelled as global constants.which result from the simulated time series, in general take more moderate values, theyare smaller than ^+. Thus, the resulting simulated con dence intervals I ^ j cover the+parametFer^+ less often. For the annualized volatilities there are major di erences betweenthe relative frequencies and the presumed 95%-level of the con dence intervals. Especiallyothe very low level shortly before 1996 is striking. This is exactly the period where there is theunexplainable doruble-peak of the volatilities (cf. Figure 4.1 and the comments thereon), andthus should not be too surprising. Note that, although we have taken J = 1000, of coursethe quantity Jt is less Pthan that for all t, due to the fact that also during the simulationsthe normal approximation efor the tails is used. However, except for very few points of time,we always have Jt 300.erThe model with globally constant parameters u+ and u . In our rst discussionRof the estimates for the parameter functions u+ and u (see p. 10), we pointed out that analternative approach might be to model theesetwo parameters by global constants. We wantto brie y investigate how this simpli ed modelling approach behaves. To this end we take theestimated scale parameters ^+ and ^ from the avbove local likelihood estimation and then werst re-estimate u+ and u as global constants, usiniga maximum likelihood (ML) estimator.Based on these ML estimates, then the parameter funections + and are re-estimated withlocal likelihood estimates, using again local linear ts. Cwonsidering the S&P 500 data thisyields globally constant parameter estimates u^g+lob = 0:209 and u^glob = 0:315, the newlyOestimated parameter functions ^+ and ^ resemble very much the corresponding functionsfrom Figure 4.1, the plots are therefore omitted. The same model assessment proceduresnthat are used above can now be applied to the standardized returns resulting from these newparameter estimates: the assessments of the SACFs and the Portmanteau tests look exactlythe same as those from Figures 4.3 and 4.5 (and are therefore omittedl),thus indicatingythat the parameter estimates describe the data quite well. This is further supported bythe p-values and the Monte Carlo p-values based on J = 1000 simulated time series ofthe goodness of t tests, which are given in Table 4.2. The p-values from the simpli edmodelling approach even seem to slightly outperform the results from Table 4.1.Of course, the above mixed modelling approach, where global constant ts for u+ andu and local linear ts for + and are used, comes with the drawback that nearly twotimes as many parameter estimates are needed in order to t the model. However, it hasthe advantage that if once tted the model can be updated more easily, because onlythe scale parameters + and have to be updated regularly, which thus also saves oncomputing time. This is in particular interesting for real-time observation of a time series.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufPage 18 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS171234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859605Forecasting with local likelihood methodsWe now want to provide an approach for forecasting future return distributions, the basicidea of which is the same as in Drees and St ric  (2002). Subsequently, we provide numericalresults for one-day-ahead distributional forecasts, and discuss them.Distributional forecasts in the regression model. Assume we are given the dataavailable up to time t which we interprete as the present and we try to forecast thedistribution of the next d-day return Xt;d = Pid=1 Xt+i. In our model this is given by8 d>>< Xt;d = d + X Rt+s;s=1G (t+s);(5.1) Fo >:> Rt+s Rt+s independent, s = 1; : : : ; d.Thus, we have tornd reasonable estimates for the future parameter values (t + s) that arebased only on the data available at time t. Presuming the forecasting horizon d is not toolarge, an extrapolation Pof the Taylor polynomial in (3.1) using the notation from Section 3,eespecially the vector notation from (3.2) yields(5.2) k(t + s) Xjp=k0 k(jje)!(t)sj Xjp=k0 kj (t)sj ; s = 1; : : : ; d; k = 1; : : : ; 4:rThe unknown coe cients kj (t) can be estimated by local likelihood methods. Therefor weemploy a one-sided kernel, i.e. we use RK() f 0g instead of K( ). Again, the estimationsare based on the centered returns, which in the forecasting context are given by R~s = Xs Xt,s t, where Xt = 1=t Pit=1 Xi. (Note that ethese centered returns have to be re-computedfor di erent values of t.) These two modi cativonsassure that the resulting local likelihoodestimates ^kj (t) only depend on the information available at time t. Plugging these initialiestimates into (5.2), we get forecasts for the future pearameters by(5.3) ^k;t(t + s) = Xpk ^kj (t)sj ; s = 1; : : : ; wd;k = 1; : : : ; 4:j=0Here and in the following the additional subscript t shall empOhasize that the parameterforecast is only based on the data up to time t. Instead of (5.3), one might also take(5.4) ^k;t(t + s) = ^k0(t); s = 1; : : : ; d; k = 1; : : : ; 4n;lthus neglecting any trend components, which express through the componenyts^kj (t), j > 0.In the case of tting each parameter by local constants, i.e. p1 = = p4 = 0, then of course(5.3) and (5.4) coincide. Replacing the unknown quantities and (t + s) in (5.1) by Xtand ^t(t + s) (either given by (5.3) or by (5.4)), the law ofd(5.5) X~t;d = dXt + X R~t+s; R~t+s G^t(t+s);s=1provides a distributional forecast for Xt;d. In the following, the correspondig cdf's of Xt;dand X~t;d are denoted Ft;d and F~t;d, respectively.The asymptotic results for the local likelihood estimates presented in Section 3 act on theassumption of a symmetric kernel, and hence seem not suitable for our forecasting purpose.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960(a) All four parameters are tted by local econstants, i.e. p1 =(b) Local linear ts are used for each of the fvourparameters, i.e. p1 =For each of the two tting schemes (a) and (b) we imiplement the cross-validation method (cf.eSection 3), computing the corresponding cross-validation functions for bandwidths rangingfrom 0:040 to 0:200. In case of tting scheme (a) the resuwlting function is close to constant,with a maximum at hCV 0:13, which is however not very distinct. For tting scheme(b) the corresponding cross-validation function is strictly increasOingfor all considered band-widths. On the basis of these results it seems hard to choose an appropriate bandwidth, inboth cases the cross-validation method yields large bandwidths. Indened,it is plausible thatfor the prediction of future parameters a larger degree of smoothing is needed than for thetting of the model. However, in case of tting scheme (b) this is somelwhat critical: theybigger we choose h the more restrictive becomes the second constraint in (4.1), and thus thesmaller in absolute value the estimates ^k(1) for the rst derivatives of the parameter functionshave to be, which runs contrary to the idea of local linear ts for the parameter functions.Therefore, in this situation it seems questionable if cross-validation is the best approach forthe choice of an appropriate bandwidth. Instead, we choose a smaller bandwidth for ourcomputations, taking h = 0:095, which is then rescaled for each t, as described above. Thisbandwidth choice is essentially based on trial and error, doing parameter estimations withdi erent bandwidths.We use the one-sided version of the Epanechnikov kernel. Otherwise, the same settingi.e. same parameter space, optimization procedure, etc. as in the simple model tting= p4 = 1.However, they can easily be carried over to our forecasting context. To this end, for eachxed t we interpret t as the right boundary point of the interval [1; t] and we do the rescalingproposed in (3.4) with n replaced by t. The bandwidth is rescaled in a similiar way, taking~h = h n=t for the estimation of (t), with h xed. The rescalation of the bandwidth ensuresthat the number of observations used for the estimation does not change with varying t.With this interpretation of t as right boundary point, it makes no di erence if we use theabove one-sided kernel or its two-sided version, because there are no observations at timeslarger than t that could have in uence on the estimate. Consequently, the asymptotics,which hold true for the right boundary point t when using a two-sided kernel, do so forthe use of the one-sided kernel as well. That is, the estimates ^(t) are consistent andasymptoFtically normal, which enables us to provide approximate con dence intervals for theotrue parameters (as proposed in Section 3). Again, it is favourable to use local polynomialsof the same degree for all parameters, i.e. p1 = = p4, in order to keep the expressionfor the asymptotricbias to a minimum. For the tting of the model it was also preferablePto chose odd degree local polynomials to ensure that the expression for the bias is of thesame order for boundary points and inner points. Since in this situation we always considerthe estimation of the pareameter function in a boundary point, the question whether thedetails). epi's should be even or odd is of minor importance (cf. Aerts and Claeskens, 1997 for moreWe apply the above forecastinrgapproach to the S&P 500 daily returns, and we do one-Rday-ahead distributional forecasts, i.e. we take d = 1. Two di erent computations are done,each using a di erent degree of local polynomial tting, namely:= p4 = 0.18LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 19 of 25E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf0.90.80.70.60.50.40.30.20.100.030.02F50.020.0150.010.0050or1992 1994 1996 1998 2000 2002u^+P1992 1994 1996 1998 2000 2002^+eer0.30.250.20.150.10.050Page 20 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS191234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859600.90.80.70.60.50.40.30.20.100.030.0250.020.0150.010.00501992 1994 1996 1998 2000 2002u^1992 1994 1996 1998 2000 2002^Rev1992 1994 1996 1998 2000 2002^a ieFigure 5.1. The estimated parameter functions u^+, u^ , ^+ and ^ , and the resultingannualized volatilities ^a for the case of local constant ts w(case (a)). The estimated functionsare depicted in solid lines, the corresponding approximate pointwise 95%-con dence intervals(where applicable) are depicted in dotted lines. Onapproach is used (cf. Section 4). Due to the use of a one-sided kernel and the speci c choiceof bandwidth we do not compute local likelihood estimates for t 300. lyNumerical results for the forecasts. Figure 5.1 shows the local likelihood estimates^k;t(t) for t = 301; : : : ; 3062, and the corresponding 95%-con dence intervals (where appli-cable) for the case of local constant ts of all four parameters (case (a)). The estimatedfunctions, and thus the estimated distributions, seem to exhibit the same symmetric be-haviour that was already discussed in Section 4. Furthermore, for almost all points of time,the left tails i.e. the tails modelling the losses are heavier than the right tails, a desirableproperty, as mentioned above. Despite the much larger bandwidth, the estimated functionsin Figure 5.1 exhibit greater uctuations on a small time scale than the corresponding esti-mates for the tting of the model (Figure 4.1). This is due to the use of the one-sided kernel.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596020LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 21 of 25We omit plots of the estimated parameter functions for case (b): in this case the estimatedfunctions look similar to those from Figure 4.1, however they are much more wiggly, andthe con dence intervals are wider.For the prediction of the law of the one-day-ahead returns Xt;1 we consider three di erentforecasting approaches. These di er with respect to the degrees of the local polynomialsthat are used for the initial likelihood estimations (represented by one of the above cases(a) and (b)), and with respect to the kind of extrapolation that is used for predicting futuredistribution parameters (either (5.3) or (5.4)):F1: The initial local likelihood estimates are computed with local constant ts for allFparameters. In this case (5.3) and (5.4) are the same.oF2: For the initial likelihood estimates local linear ts are used for all parameters, theparameter forecasts are given by (5.3).rF3: Same as F2, but with parameter forecasts according to (5.4).PWe now want to analyse the quality of the distributional forecasts provided by the randomvariables X~t;1. Essentially, ethe analysis is based on the same ideas and testing devices thatwe used for the assessments of the quality of the model tting in Section 4. Suppose forea moment that the forecasting distributions L (X~t;1) perfectly match the laws of the truefuture returns L (Xt;1), i.e. for trhecorresponding cdf's we have F~t;1(x) = Ft;1(x) for allx 2 . Then the variablesR(5.6) Zt;1 := 1 F~t;1(Xt;1) ; t = 301; : : : ; 3061;ein the following referred to as standardized vforecasts should be i.i.d. standard normal.To check whether this holds true, we have to test for both, independence and normality ofthe standardized forecasts. We do so for each of thieforecasting methods F1 F3.eIn a rst step to check for the normality of the standardized forecasts, we apply thegoodness of t tests (KS, JB ans SW tests) to the time sewriesfZt;1 : t = 301; : : : ; 3061g. Thecorresponding p-values for each of the three forecasting approaches are given in Table 5.1.Considering only the p-values resulting from the simple versions Oof the three goodness oft tests does not allow for an answer to the question whether the standardized forecastsare (at least approximately) standard normal: as can be seen from tnhetable, the KS testsupports the hypothesis of normality of the standardized forecasts for all three forecastingapproaches, whereas the JB and SW tests nearly always reject the hypothlesyis.This con ictmight be a consequence of the standardization (5.6) and its e ects on the values of thedi erent test statistics (compare the discussion in Section 4), thus indicating the inadequacyof these simple goodness of t tests. For this reason, again, we consider Monte Carlotests. The basic idea is the same as described in Section 4: we simulate J time seriesfR~t j : t = 301; : : : ; 3062g, with independently simulated random variables R~t j G^t(t).Based on these simulated time series we re-compute the initial local likelihood estimatesf ^ j(t) : t = 601; : : : ; 3062g, and we compute the corresponding forecasting parameters^t j (t + 1) and the simulated standardized forecasts fZt;j1 : t = 601; : : : ; 3061g. Repeatingthese operations for j = 1; : : : ; J , and computing the KS, JB and SW test statistics forthese simulated standardized forecasts, nally yields approximate Monte Carlo p-values.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufForecasting methodF1F2F3TestppMCppMCppMCKolmogorov-Smirnov0.9690.9020.2350.9060.2440.902Jarque-Bera0.0030.6560.0000.7030.0000.701Shapiro-Wilk0.0540.7500.0000.7440.0000.741Table 5.1. The p-values and Monte Carlo p-values for the KS, JB and SW tests for thestandardized forecasts resulting from the three forecasting methods F1 F3. The Monte Carlop-values Fare based on 1000 simulated time series.or43lise 2tsscaaqu 01 Ptnreirezadd-2 efo-1tand-3S-4 e--54 -3 -2Stand-a1rd nor0mal qu1antilesr23 46lse 4itan 2uqtssa 0rcefod-2zedirad-4ntaS-6--84 -3 -2Stand-a1rd nor0mal qu1antiles2 3 4Page 22 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS21123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960Figure 5.2. QQ-plots for the standaRrdized forecasts fZt;1 : t = 301; : : : ; 3061g resultingfrom the forecasting approaches F1 (left) and F3 (right).evWe do these Monte Carlo tests for each of the forecasting approaches F1 F3, where eachof the computed pMC-values is based on J = 1000isimulated time series. The results areegiven in Table 5.1, too. Without exception, the tests strongly support the hypothesis ofnormality of the standardized forecasts, and thus the idea wofa good forecasting. Apparently,there are only small di erences in the pMC-values for Methods F2 and F3. Thus, for thequality of one-day-ahead parameter forecasts it seems not of great Oimportance whether theseincorporate a trend component or not. This is due to the fact that in our special case thevalues of the forecasting parameters ^k;t(t + 1) only vary at an order nofmagnitude of 10 3,depending on whether they are computed according to (5.3) or (5.4). Consequently, for theone-day-ahead distributional forecasts it has nearly no e ects which of tlhese two methodsyis used for the computation of ^k;t(t + 1). For the same reason one can anticipate that theforecasting results will not improve signi cantly by chosing higher order polynomial ts, andtherefore for one-day-ahead forecasts the use of local constant ts seems favourable sinceit is computationally least intensive.Next we consider the QQ-plots of the standardized forecasts. For the cases F1 and F3these are depicted in Figure 5.2. (The QQ-plot for the standardized forecasts based on fore-casting method F2 looks very similar to that of F3, so we omit it.) As can be seen from thegure, the data roughly t the main diagonal, thus underpinning the normality hypothesisfor the standardized forecasts. Solely in the tails there seem to be some deviations, especiallywhen considering the standardized forecasts resulting from method F3 (Figure 5.2, right).E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596022LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 23 of 250.10.080.060.04FC 0.02SA 0-0.02-0.04-0.06 0 10 20 30 40 L5a0g 60 70 80 90 1000.10.080.060.04FC 0.02SA 0-0.02-0.04-0.06 0 10 20 30 40 L5a0g 60 70 80 90 100Figure 5.3. The SACFs of the standardized forecasts fZt;1 : t = 301; : : : ; 3061g (left) andof the absolute values fjZt;1j : t = 301; : : : ; 3061g (right), each for the rst 100 lags. ThestandardFized forecasts are based on the forecasting approach F1. The dashed lines representthe 95%-con dence intervals.orAltogether, taking into account both the QQ-plots and the results of the Monte Carlogoodness of t tests, Pthedata seem not to directly contradict the hypothesis of normal-ity of the standardized forecasts regardless of which forecasting approach F1, F2 or F3eis considered. Thus, at least approximately the standardized forecasts can be considerednormal. This supports the asseumption that the distributional one-day-ahead forecasts F~t;1are reasonable approximations for rthe true distributions Ft;1.It remains to investigate the dependence structure of the standardized forecasts. Tothis end, we consider the SACFs of the Rtime series fZt;1g and fjZt;1jg. For the forecastingeapproach F1 these are depicted examplarily in Figure 5.3. The corresponding SACFs for thestandardized forecasts based on the other forecasting approaches look very similar to theones showed in Figure 5.3 and thus are omittedv.Under the assumption of independence ofthe standardized forecasts, the absolute values of bioth SACFs should be very small for alllags. The SACF of the standardized forecasts fZt;1g eclearly exhibits such a behaviour andthus might be evidence for the independence assumption wtobe true. However, the SACFof the time series fjZt;1jg clearly seems to di er from that. For about the rst 30 lags theSACF exceeds the upper 95%-con dence bound. In addition, the Portmanteau tests forboth, the standardized forecasts as well as for their absolute vaOlues, reject the hypothesisof independence for almost all lags. The corresponding p-value plots look exactly the sameas the top right image in Figure 4.3, and thus are omitted. (The nstatements just madealso hold true for the other forecasting approaches F2 and F3.) Thus, thelhypothesis of thestandardized forecasts to be independent seems a bit questionable. yForecasting over longer horizons. Of course the forecasting approach that we usedabove can also be used for larger forecasting horizons, say d = 20 or d = 40. Drees andSt ric  (2002) do such examinations. However, with a larger forecasting horizon thereoccur several di culties: one major problem is that the distributions of the forecastingvariables X~t;d cannot be described in a closed form. Hence, simulations are needed to ndgood approximations for the convolution given in (5.5). Furthermore, the variables Xt;dare not independent. Even in case of the returns being independent which is a somewhatcritical assumption, as we have seen in the above examinations the forecasting variablesE-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rqufPage 24 of 25LOCAL LIKELIHOOD ESTIMATORS IN A QREuGaRnEtiStSaItOivNeMFOinDaELncFeOR STOCK RETURNS23123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960form a (d 1)-dependent sequence of random variables. And the same holds true for thecorresponding standardized d-day forecasts Zt;d = 1 F~t;d(Xt;d) . Hence, the basis for anapplication of the goodness of t tests is clearly violated. (Remember that this was alreadycritical for the one-day-ahead forecasts!) On the other hand Monte Carlo tests, like we didthem above, are computationally way too expensive, because of the additional simulationsneeded for computing the distributions of the variables Xt;d. Hence, an assessment of theforecasts becomes a di cult task. For these reasons we only consider one-day-ahead forecastsin this paper.6FConcluding remarksoBased on a work rof Drees and St ric  (2002), we considered a simple regression type modelfor stock returns. The Pinnovations were modelled by four parameter asymmetric distribu-tions, and the parameters were modelled as smooth, deterministic functions of time. Inaddition, we considered aenappropriate normal approximation for the tails, enabling themodel to adapt to both, heaevy-tailed as well as light-tailed return distributions. By therexample of the S&P 500 daily returns, observed over a period of twelve years, the principleof local likelihood estimation (in connection with local linear ts) prove to be a reasonablemethod for tting our model to real-Rlifedata. In addition, approximate con dence inter-vals for the true parameters could be given. The assessment of the model via the (MonteCarlo) KS, JB and SW tests argued for a geood tting of our estimated model to the givendata. Solely the assumption of independence of vthe innovations seemed somewhat critical:although the investigation of the SACFs of the (standardized) returns and their absolute val-ues supported the hypothesis of independent innoviateions, the outcome of the Portmanteautests partly spoke against that. We also considered a simple device for forecasting future re-wturn distributions, and we did one-day-ahead distributional forecasts, using local likelihoodestimators with several forecasting approaches. Actually, an evaluation of these estimatespointed out some minor aws, concerning the independence assOumption of the returns andthe results of the simple versions of the goodness of t tests. However, the results of theMonte Carlo tests as well as the QQ-plots for the standardized forecnasts clearly argued forour modelling approach to yield reasonable distributional forecasts.lSummarizing, one can say that our simple modelling approach, in connyection with thelocal likelihood estimators, seems to succeed in describing nancial time series and givingreasonable one-day-ahead forecasts. The results clearly underpin the adequacy of the ideaof a local modelling approach.Acknowledgements. I would like to thank Holger Drees for his guidance and encour-agement. I am also grateful to the referees for their valuable comments, which led to animprovement of this paper. This work was made possible by the nancial support of DeutscheForschungsgemeinschaft (DFG).E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596024LOCAL LIKELIHOOD ESTQIMuAaTnOtRitSatIiNveA FRiEnGaRnEcSeSION MODEL FOR STOCK RETURNSPage 25 of 25References<ref>Aerts, M. and Claeskens, G. (1997). Local polynomial estimation in multiparameter likeli-</ref>hood models. Journal of the American Statistical Association, 92:1536 1545.<ref>Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of</ref>Econometrics, 31:307 327.<ref>Bollerslev, T., Engle, R., and Nelson, D. (1994). ARCH models. In Engle, R. and McFadden,</ref>D., editors, Handbook of Econometrics, vol. 4, pages 2961 3038. Elsevier.<ref>Brockwell, P. and Davis, R. (1987). Time Series Analysis: Theory and Methods. Springer,</ref>New YFork.<ref>Davison, A. oandHinkley, D. (2003). Bootstrap Methods and their Application. Cambridge</ref>University Press, Cambridge.r<ref>Drees, H. and St ric , C. (2002). A simple non-stationary model for stock returns. Preprint,</ref>available at www.maPth.chalmers.se/ starica.<ref>Engle, R. (1982). Autoregeressive conditional heteroskedasticity with estimates of the vari-</ref>ance of UK in ation. Econoemetrica, 50:987 1008.<ref>Fan, J., Farmen, M., and Gijbelrs,I. (1998). Local maximum likelihood estimation and</ref>inference. Journal of the Royal Statistical Society, series B, 60(3):591 608.R<ref></ref>ations. Chapman</ref>& Hall, London. eFan, J. and Yao, Q. (2003). Nonlinear Time Sevries. Springer, New York.Gouriéroux, G. (1997). ARCH Models and Financiial Applications. Springer, New York.emarbeit, Universität Hamburg. wJönck, U. (2005). Lokale Likelihood-verfahren zur Modellierung von Finanzzeitreihen. Diplo-Johnson, N. and Kotz, S. (1970). Continuous Univariate Distributions, vol. 2. HoughtonMi in, Boston. OJudge, G., Hill, R., Gri ths, W., Lütkepohl, H., and Lee, T.-C. (19n88). Introduction thothe Theory and Practice of Econometrics. Wiley, New York.lMikosch, T. and St ric , C. (2003). Long-range dependence e ects and ARyCH modelling.In Doukhan, P., Oppenheim, G., and Taqqu, M., editors, Theory and Applications ofLong-range Dependence, pages 439 459. Birkhäuser, Boston.Mikosch, T. and St ric , C. (2004). Non-stationarities in nancial time series, the long rangedependence and the IGARCH e ect. Review of Economics and Statistics, 86:378 390.Shapiro, S. and Wilk, M. (1965). An analysis of variance test for normality (completesamples). Biometrika, 52:591 611.St ric , C. (2003). Is GARCH(1,1) as good a model as the Nobel prize accolades wouldimply? Preprint, available at www.math.chalmers.se/ starica.E-mail: quant@tandf.co.uk URL://http.manuscriptcentral.com/tandf/rquf