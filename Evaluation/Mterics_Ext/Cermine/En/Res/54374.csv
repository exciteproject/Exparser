Survey Research Methods (2017)Vol. 11, No. 1, pp. 17-44doi:10.18148/srm/2017.v11i1.7149c European Survey Research AssociationISSN 1864-3361http://www.surveymethods.orgBias and e ciency loss in regression estimates due to duplicatedobservations: a Monte Carlo simulationFrancesco SarracinoInstitut National de la Statistique et des ÉtudesÉconomiques du Grand-Duché du Luxembourg (STATEC)Luxembourg, LuxembourgandLCSR National Research UniversityHigher School of EconomicsMoscow, Russian FederationMałgorzata MikuckaMannheimer Zentrum für EuropäischeSozialforschung (MZES)Mannheim, GermanyandLCSR National Research UniversityHigher School of EconomicsMoscow, Russian FederationRecent studies documented that survey data contain duplicate records. In this paper, we assesshow duplicate records a ect regression estimates, and we evaluate the e ectiveness of solutionsto deal with them. Results show that duplicates bias the estimated coe cients and standard er-rors. The chances of obtaining unbiased estimates when data contain 40 doublets (about 5% ofthe sample) range between 3.5% and 11.5% depending on the distribution of duplicates. If 7quintuplets are present in the data (2% of the sample), then the probability of obtaining biasedestimates ranges between 11% and 20%. Weighting the duplicate records by the inverse oftheir multiplicity, or dropping superﬂuous duplicates outperform other solutions in all consid-ered scenarios in reducing the bias and the risk of obtaining biased estimates. However, bothsolutions overestimate standard errors, reducing the statistical power of estimates. Our studyillustrates the risk of using data in presence of duplicate records and call for further researchon strategies to analyse a ected data.Keywords: Duplicate records, Estimation Bias, Monte Carlo Simulation, Inference, SurveyData Quality1 IntroductionTo achieve reliable results survey data must accuratelyreport respondents’ answers. Yet, sometimes they don’t.The study of Slomczynski, Powałko, and Krauze (2017)published in this issue of SRM investigated survey projectswidely used in social sciences, and reported a considerablenumber of duplicate records in 17 out of 22 internationalprojects. Duplicate records are deﬁned as records that arenot unique, that is records in which the set of all (or nearlyall) answers from a given respondent is identical to that ofanother respondent.Surveys in social sciences usually include a large num-ber of questions, and it is unlikely that two respondents pro-vide identical answers to all (or nearly all) substantive surveyquestions (Hill, 1999). In other words, it is unlikely that twoidentical records originate from the answers of two real re-spondents. It is more probable that one record corresponds toa real respondent and the second one is its duplicate, or thatContact information: Francesco Sarracino, STATEC, 13 rueErasme, L-2013 Luxembourg (email: f.sarracino@gmail.com)17both records are fakes. Duplicate records can result from anerror or forgery by interviewers, data coders, or data pro-cessing sta and should, therefore, be treated as suspiciousobservations (American Statistical Association, 2004; Diek-mann, 2005; Koczela, Furlong, McCarthy, & Mushtaq, 2015;Kuriakose & Robbins, 2016; Waller, 2013).1.1Duplicate records in social survey dataSlomczynski et al. (2017) analyzed 1,721 national surveysbelonging to 22 comparative survey projects, with data com-ing from 142 countries and nearly 2.3 million respondents.The analysis identiﬁed 5,893 duplicate records in 162 na-tional surveys from 17 projects coming from 80 countries.The duplicate records were unequally distributed across thesurveys. For example, they appeared in 19.6% of surveys ofthe World Values Survey (waves 1–5) and in 3.4% of surveysof the European Social Survey (waves 1–6). Across surveyprojects, di erent numbers of countries were a ected. Lati-nobarometro is an extreme case where surveys from 13 outof 19 countries contained duplicate records. In the AmericasBarometer 10 out of 24 countries were a ected, and in the In-ternational Social Survey Programme 19 out of 53 countriescontained duplicate records.18FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKAEven though the share of duplicate records in most sur-veys did not exceed 1%, in some of the national surveys itwas high, exceeding 10% of the sample. In 52% of the af-fected surveys Slomczynski et al. (2017) found only a sin-gle pair of duplicate records. However, in 48% of surveyscontaining duplicates they found various patterns of dupli-cate records, such as multiple doublets (i.e. multiple pairsof identical records) or identical records repeated three, four,or more times. For instance, the authors identiﬁed 733 du-plicate records (60% of the sample), including 272 doubletsand 63 triplets in the Ecuadorian sample of Latinobarometrocollected in the year 2000. Another example are data fromNorway registered by the International Social Survey Pro-gramme in 2009, where 54 duplicate records consisted of27 doublets, 36 duplicate records consisted of 12 triplets,24 consisted of 6 quadruplets, 25 consisted of 5 quintuplets;along with, one sextuplet, one septuplet, and one octuplet(overall 160 duplicate records, i.e. 11.0% of the sample).These ﬁgures refer to full duplicates. However, other re-search analyzed the prevalence of near duplicates, that isrecords which di er for only a small number of variables.Kuriakose and Robbins (2016) analyzed near duplicates indata sets commonly used in social sciences and showed that16% of analysed surveys reported a high risk of widespreadfalsiﬁcation with near duplicates. The authors emphasizedthat demographic and geographical variables are rarely falsi-ﬁed, because they usually have to meet the sampling frame.Behavioral and attitudinal variables, on the other hand, werefalsiﬁed more often. In such cases, interviewers may onlycopy selected sequences of answers from other respondents,so that the correlations between variables are as expected,and the forgery remains undetected.1.2 Implications for estimation resultsDuplicate records may a ect statistical inference in var-ious ways. If duplicate records introduce “random noise”,then they may produce an attenuation bias, i.e. bias the es-timated coe cient towards zero (Finn & Ranchhod, 2013).However, if the duplicate records do not introduce randomnoise, they may bias the estimated correlations in other direc-tions. The size of the bias should increase with the numberof duplicate interviews, and it should depend on the di er-ence of covariances and averages between the original andduplicate interviews (Schräpler & Wagner, 2005).On the other hand, duplicate records can reduce the vari-ance, and thus they may artiﬁcially increase the statisticalpower of estimation techniques. The result is the opposite ofthe attenuation bias: narrower estimated conﬁdence intervalsand stronger estimated relationships among variables (Kuri-akose & Robbins, 2016). In turn, this may increase the statis-tical signiﬁcance of the coe cients and a ect the substantiveconclusions.The implications of duplicates for regression estimatesmay di er according to the characteristics of the observa-tions being duplicated. Slomczynski et al. (2017) suggestedthat “typical” cases, i.e. the duplicate records located nearthe median of a variable, may a ect estimates less than “de-viant” cases, i.e. duplicate records located close to the ties ofthe distribution.The literature on how duplicate records a ect estimatesfrom regression analysis, and how to deal with them is virtu-ally not existing. Past studies focused mainly on strategies toidentify duplicate and near-duplicate records (Elmagarmid,Ipeirotis, & Verykios, 2007; Hassanzadeh & Miller, 2009;Kuriakose & Robbins, 2016; Schreiner, Pennie, & New-brough, 1988). However some studies analyzed how inten-tionally falsiﬁed interviews (other than duplicates) a ectedsummary statistics and estimation results. Schnell (1991)studied the consequences of including purposefully falsiﬁedinterviews in the 1988 German General Social Survey (ALL-BUS). The results showed a negligible impact on the meanand standard deviation of variables. However, the falsiﬁedresponses produced stronger correlations between objectiveand subjective measures, more consistent scales (with higherCronbach’s ), higher R2, and more signiﬁcant predictorsin OLS regression. More recently, Schräpler and Wagner(2005) and Finn and Ranchhod (2013) did not conﬁrm thegreater consistency of falsiﬁed data. On the contrary, theyshowed a negligible e ect of falsiﬁed interviews on estima-tion bias and e ciency.1.3Current analysisOur study is the ﬁrst analysis of how duplicate recordsa ect the bias and e ciency of regression estimates. We fo-cus on two research questions: ﬁrst, how do duplicates a ectregression estimates? Second, how e ective are the possiblesolutions? We use a Monte Carlo simulation, a techniquefor generating random samples on a computer to study theconsequences of probabilistic events (Ferrarini, 2011; Fish-man, 2005). In our simulations we consider three scenariosof duplicate data:Scenario 1. when one record is multiplied several times (asextuplet, an octuplet, and a decuplet),Scenario 2. when several records are duplicated once (16,40, and 79 doublets, which correspond to 2%, 5% and10% of the sample respectively),Scenario 3. when several records are duplicated four times(7, 16, and 31 quintuplets, which correspond to 2%,5% and 10% of the sample).We chose the number of duplicates to mimic the results pro-vided by Slomczynski et al. (2017). We also investigate howregression estimates change when duplicates are located inspeciﬁc parts of the distribution of the dependent variable.We evaluate four variants, namely:BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS19Variant i. when the duplicate records are chosen randomlyfrom the whole distribution of the dependent variable(we label this variant “unconstrained” as we do not im-pose any limitation on where the duplicate records arelocated);Variant ii. when they are chosen randomly between the ﬁrstand third quartile of the dependent variable (i.e. whenthey are located around the median: this is the “typi-cal” variant);Variant iii. when they are chosen randomly below the ﬁrstquartile of the dependent variable (this is the ﬁrst “de-viant” variant);Variant iv. when they are chosen randomly above the thirdquartile of the dependent variable (this is the second“deviant” variant).We expect, consistently with the suggestion by Slomczynskiet al. (2017), that Variants iii and iv a ect regression esti-mates more than Variant i, and that Variant ii a ects themthe least. Additionally, we repeat the whole analysis to testthe robustness of our ﬁndings by checking how the positionon the distribution of one of the independent variables a ectsregression estimates.For each scenario and variant we compute the follow-ing measures to assess how duplicates a ect regression es-timates:Measure A. percentage bias of coe cients;Measure B. bias of the standard errors;Measure C. risk of obtaining biased estimates, as measuredby Dfbetas;Measure D. Root Mean Square Error (RMSE), which in-forms about the e ciency of the estimates.We consider ﬁve solutions to deal with duplicate records,and we assess their ability to reduce the bias and the e -ciency loss:Solution a. “naive” estimation, i.e. analysing the data as ifthey were correct;Solution b. dropping all the duplicates from the data;Solution c. ﬂagging the duplicate records and including theﬂag among the predictors;Solution d. dropping all superﬂuous duplicates;Solution e. weighting the duplicate records by the inverse oftheir multiplicity.Finally, we check the sensitivity of our results to the sam-ple size. Our basic analysis uses a sample of N = 1; 500,because many nationally representative surveys provide sam-ples of similar sizes. However, we also run the simulation forsamples of N = 500 and N = 5; 000 to check the robustnessof our results to the chosen sample sizes.Table 1Matrix of correlations used to generate the original data set.variablesxztx1:000:040:09z1:000:06t1:002MethodTo assess how duplicate records a ect the results of OLSregression we use a Monte Carlo simulation (Ferrarini, 2011;Fishman, 2005). The reason is that we need an artiﬁcial dataset where the relationships among variables are known, andin which we iteratively manipulate the number and distribu-tion of duplicates. The random element in our simulationis the choice of records to be duplicated, and the choice ofobservations which are replaced by duplicates. At each iter-ation, we compare the regression coe cients in presence ofduplicates with the true coe cients (derived from data with-out duplicates) to tell whether duplicates a ect regression es-timates.Our analysis consists of four steps. First, we generate theinitial data set. Second, we duplicate randomly selected ob-servations according to the three scenarios and four variantsmentioned above. In the third step we estimate regressionmodels using a “naive” approach, i.e. treating data with du-plicates as if they were correct (Solution a). In the same stepwe also estimate regression models using the four alternativesolutions (b–e) to deal with duplicate records. Finally, wecompute the bias of coe cients and standard errors, the riskof obtaining biased estimates, and the Root Mean Square Er-ror to assess the e ect of duplicates on regression estimatesand the e ectiveness of the solutions. Figure 1 summarizesour strategy.2.1Data generationWe begin by generating a data set of N = 1; 500 observa-tions which contains three variables: x, z, and t. We createthe original data set using random normally distributed vari-ables with a known correlation matrix (shown in Table 1).The correlation matrix is meant to mimic real survey dataand it is based on the correlation of household income, age,and number of hours worked as retrieved from the sixth waveof the European Social Survey (2015).We generate the dependent variable (y) as a linear functionof x, z, and t as reported in Equation 1:yi = 5:360:04 xi + 0:16 zi + 0:023 ti + i(1)where the coe cients are also retrieved from the sixth waveof the<ref> European Social Survey. All variables and the errort</ref>erm i are normally distributed. The descriptive statisticsData generation:Scenarios:Variants:Solutions:(2,500 replications)20FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKAoneoriginaldata set,(N=1500)1.1 observation duplicated5, 7, and 9 times (sextu-plet, octuplet, decuplet)2.16, 40, and 79 ob-servations duplicated1 time (doublets)3.7, 16, and 31 obser-vations duplicated 4times (quintuplets)i.Unconstrained: ran-domly drawn from theoverall distributionii.Typical: ran-domly drawn fromaround the medianiii.Deviant: randomlydrawn from theupper quartileiv.Deviant: randomlydrawn from thelower quartilea.“naive”estimationb.excluding allduplicatesc.duplicatesﬂagged andcontrolled ford.excludingsuperﬂousduplicatese.weighted bythe inverse ofmultiplicities(1 regression model per replication)Measures of biasand e ciency:A.Bias of co-e cientsB.Bias of stan-dard errorsFigure 1. Diagram summarizing the empirical strategy.of the generated data set are shown in the ﬁrst four lines ofTable A1 in Appendix A.2.2Duplicating selected observationsIn the second step we use a Monte Carlo simulation togenerate duplicate records, which replace for randomly cho-sen original records, i.e. the interviews that would have beenconducted if no duplicates had been introduced in the data.This strategy is motivated by the assumption that duplicaterecords substitute for authentic interviews. Thus, if dupli-cates are present, researchers do not only face the risk offake or erroneous information, but they also lose informa-C.Risk of ob-taining biasedestimatesD.RMSEtion from genuine respondents. We duplicate selected ob-servations in three scenarios (each comprising three cases)and in four variants. Thus, overall we investigate 36 patterns(3 3 4 = 36) of duplicate records. For each pattern we run2,500 replications.Scenario 1: a sextuplet, an octuplet, and a decuplet.In the ﬁrst scenario we duplicate one randomly chosen record5, 7, and 9 times, thus introducing in the data a sextuplet,an octuplet, and a decuplet of identical observations whichreplace for 5, 7, and 9 randomly chosen original observa-tions. These cases are possible in the light of the analysis bySlomczynski et al. (2017) who identiﬁed in real survey dataBIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS21Variant i: unconstrainedinstances of octuplets. In this scenario the share of duplicatesin the sample is small, ranging from 0,4% for a sextuplet to0,7% for a decuplet.Scenario 2: 16, 40, and 79 doublets. In the second sce-nario we duplicate sets of 16, 40, and 79 randomly chosenobservations one time, creating 16, 40, and 79 pairs of iden-tical observations (doublets). In this scenario the share ofduplicates is 2,1% (16 doublets), 5,3% (40 doublets), and10,5% (79 doublets). These shares are consistent with theresults by Slomczynski et al. (2017), as in their analysisabout 15% of the a ected surveys had 10% or more duplicaterecords.Scenario 3: 7, 16, and 31 quintuplets. In the third sce-nario we duplicate sets of 7, 16 and 31 randomly chosen ob-servations 4 times, creating 7, 16 and 31 quintuplets. Theyreplace, for 28, 64, and 124 randomly chosen original recordsrespectively. In this scenario the share of duplicate records is2,3% (7 quintuplets), 5,3% (16 quintuplets), and 10,3% (31quintuplets).To check whether the position of duplicates in the distri-bution matters, we run each of the scenarios in four variants,as presented in Figure 2.Variant i (“unconstrained”). The duplicates and thereplaced interviews are randomly drawn from the overall dis-tribution of the dependent variable.Variant ii (“typical”). The duplicates are randomlydrawn from the values around the median of the dependentvariable, i.e. between the ﬁrst and third quartile, and the re-placed interviews are drawn from the overall distribution.Variant iii and iv (“deviant”). In Variant iii the dupli-cates are randomly drawn from the lower quartile of the de-pendent variable; in Variant iv they are randomly drawn fromthe upper quartile of the dependent variable. The replacedinterviews are drawn from the overall distribution.To illustrate our data, Table A1 in Appendix A reports thedescriptive statistics of some of the data sets produced duringthe replications (lines 5 to 45).2.3“Naive” estimation and alternative solutionsIn the third step we run a “naive” estimation which takesdata as they are, and subsequently we investigate the four so-lutions to deal with duplicates. For each solution we estimatethe following model:yi =+ x xi + z zi + t ti + "i(2)Solution a: “naive” estimation. First, we investigatewhat happens when researchers neglect the presence of du-plicate observations. In other words, we analyze data withduplicate records as if they were correct. This allows us toestimate the percentage bias, the standard errors, the risk ofobtaining biased estimates, and the Root Mean Square Errorresulting from the mere presence of duplicate records (seeSection 2.4).fY (y)fY (y)fY (y)fY (y)Variant ii: “typical”Variant iii: “deviant”yyyyVariant iv: “deviant”Figure 2. Presentation of the Variants i–iv used in the MonteCarlo simulation.22FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKASolution b: Drop all duplicates. In Solution b we dropall duplicates, including the observations that may comefrom true interviews. We consider such a case, because, ifrecords are identical on some, but not on all variables (mostlikely, di erences may exist on demographic and geograph-ical variables to reﬂect the sampling scheme), then it is notobvious to tell the original observations from the fake dupli-cates. It is also possible that all duplicates are forged andshould be excluded from the data. Therefore, rather thatdeleting the superﬂuous duplicates and retaining the originalrecords, we exclude all duplicate records from the data at thecost of reducing the sample size.Solution c: Flag duplicated observations and controlfor them. This solution is similar to the previous one be-cause we identify all duplicate records as suspicious. How-ever, rather than dropping them, we generate a dichotomousvariable (duplicate = 1, otherwise = 0), and include it amongthe predictors in Equation 2. Slomczynski et al. (2017) pro-posed this solution as a way to control for the error generatedby duplicate records.Solution d: Drop superﬂuous duplicates.“[E]liminating duplicate and near duplicate observationsfrom analysis is imperative to ensuring valid inferences”(Kuriakose & Robbins, 2016, p. 2). Hence, we deletesuperﬂuous duplicates to retain a sample of unique records.The di erence compared to Solution b is that we keep onerecord for each set of duplicates.Solution e: Weight by the inverse of multiplicities.Lessler and Kalsbeek (1992) proposed this method. Weconstruct a weight which takes the value of 1 for uniquerecords, and the value of the inverse of multiplicity for du-plicate records. For example, the weight takes the value 0.5for doublets, 0.2 for quintuplets, 0.1 for decuplets, etc. Sub-sequently, we use these weights to estimate Equation 2.2.4The assessment of bias and e ciencyWe use four measures to assess the consequences of du-plicates for regression estimates, and to investigate the e -ciency of the solutions to deal with them.Measure A: Bias of coe cients. This macro measureof bias informs whether a coe cient is systematically overor under estimated. It is computed as follows:Bias of coe cients = BBBBB0 biBB@1CCCC 100%CCCA(3)where i indicates a speciﬁc replication,is the true coef-ﬁcient from Equation 1, and bi is the average of estimatedcoe cients (bi).Measure B: Bias of standard errors. To test whetherduplicates artiﬁcially increase the power of regression es-timates, we compute the average of the standard errors(SE(bi)) for each scenario, variant, and solution. For easeof interpretation, we express our measure as a percentage ofthe standard errors estimated in the true model (see Equation4).Bias of S.E. = BBBB0 SE(bi) 1CCCC 100%B@B SE( ) CCAMeasure C: Risk of obtaining biased estimates. It ispossible to obtain biased estimates even if the average biasis zero. This can happen if the upward and downward biaseso set each other. To assess the risk of obtaining biased es-timates in a speciﬁc replication, we resort to Dfbetas, whichare normalized measures of how much speciﬁc observations(in our case the duplicates) a ect the estimates of regressioncoe cients. Dfbetas are deﬁned as the di erence betweenthe estimated and the true coe cients, expressed in relationto the standard error of the estimated coe cient (see Equa-tion 5).8xi = <>>1 if jDfbetaij > 0:5;>>:0 otherwise.Pr(Bias) = xi 100%(4)(5)(6)(7)Dfbetai = bSE(bi)iDfbetas measure the bias of a speciﬁc estimation, thus, theycomplement percentage bias by informing about the risk ofobtaining biased estimates. The risk is computed accordingto Equation 6. We set the cuto value to 0.5, i.e. we considerthe estimation as biased if the coe cients di er from the truevalues by more than half standard deviation.1Measure D: Root Mean Square Error (RMSE). Rootmean square error is an overall measure of the quality of theprediction and it reﬂects both its bias and its dispersion (seeEquation 7).RMSE =sib2+ SE(bi) 2The RMSE is expressed in the same units of the variables(in this case the coe cients), and it has no clear-cut thresh-old value. For ease of interpretation we express RMSE asa percentage of the respective coe cients from Equation 1,thus we report the normalized RMSE.1This cuto value is more conservative than the customary as-sumed value of p2N , which, for N = 1; 500 leads to the thresholdvalue of 0.05.BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS233Results3.1Percentage biasTable 2 shows the percentage bias of the coe cient x forthe considered scenarios, variants, and solutions. The resultsfor the other coe cients are reported in Tables B1–B3 in Ap-pendix B. The third column of Table 2 contains informationabout the percentage bias for solution a, i.e. the “naive” esti-mation.Overall, the percentage bias takes values between nearlyzero (in Scenario 1 and in all Scenarios in Variant i) and about7%. For x it reaches the maximum values of 4% for 79 dou-blets and 6% for 31 quintuplets. The maximum bias for zand t is 5%–7%, and for the intercept it is 2.5%–4% (seeAppendix B).The results show some regularities. First, number andcomposition of duplicates matter. The bias systematicallyincreases with the share of duplicates in the data, and dupli-cates consisting of quintuplets produce greater bias than du-plicates consisting of doublets. Second, the choice of recordsto be duplicated plays a role. The “unconstrained” variant(Variant i), where duplicate cases are randomly selected, pro-duces virtually no bias, even when the share of duplicatesreaches 10% of the sample. On the other hand, Variant ii pro-duces similar bias as Variants iii and iv. In other words, con-trary to our expectations, the duplication of “typical” recordsproduces a bias similar to the one induced by the presenceof “deviant” duplicates. Only randomly chosen duplicatesgenerate no bias. Third, although previous studies suggestedthat duplicates may introduce “random noise” to the data,thus leading to attenuation bias, we did not ﬁnd the evidenceto support this expectation: depending on the Variant (ii–iv),for each variable the presence of duplicates induces a mix ofoverestimated and underestimated coe cients.Among the four solutions to deal with duplicates, solu-tions d and e, i.e. dropping the superﬂuous duplicates andweighting by the inverse of multiplicity perform the best inall Variants, reducing the bias to zero. On the other hand,dropping all duplicates (solution b) and ﬂagging duplicatesand controlling for them in the regression (solution c) per-form poorly. Especially in Scenario 2 both these solutionsincrease the bias of all coe cients; in Scenario 3 they reducethe bias, but to a lesser degree than solutions d and e.In sum, duplicates can systematically bias regression es-timates if they are not randomly created. However, the biasin our simulation did not exceed 10% of the true coe cients.Moreover, dropping superﬂuous duplicates or weighting bythe inverse of their multiplicity are e ective ways to reducethe bias to zero.3.2Standard errorsTo understand whether duplicates artiﬁcially increase thestatistical power of regression estimates, we inspect the av-erage estimated standard errors, as shown in Table 3 for x.The results for other coe cients are presented in Tables C1–C3 in Appendix C.The results show, similarly to the case of percentage bias,that duplicates in Variant i, i.e. randomly drawn from theoverall distribution (“unconstrained”), do not a ect the es-timates of the standard errors. In Variant ii, in which theduplicates are located around the median of the dependentvariable, the estimated standard errors are biased downwardsby maximum 2%–3%, thus the conﬁdence intervals are nar-rower than in the true model. On the contrary, in Variantsiii and iv, i.e. the two “deviant” cases, duplicates lead tostandard errors biased upwards by maximum 2%–3%, and tobroader conﬁdence intervals. Both e ects are stronger whendata contain more duplicates, i.e. when data contain 79 dou-blets or 31 quintuplets.Among the considered solutions, ﬂagging and controllingfor duplicates (Solution c) leads to systematically narrowerconﬁdence intervals. This is especially worrisome becausethe same solution produces the most biased coe cients. Inother words, this solution may result in biased and signiﬁcantcoe cients, thus a ecting the interpretation of the results.The remaining three Solutions, b, d, and e, produce slightlygreater standard errors than the naive estimation. The rela-tive performance of the solutions varies across speciﬁc coef-ﬁcients: for x and z Solution e works better than droppingduplicates, but it overestimates the standard errors more thandropping the duplicates for t and the intercept.Summing up, we ﬁnd no evidence that the duplicates ar-tiﬁcially increase the statistical power of the estimates if theduplicates are created randomly. However, if duplicates arechosen from the center of the distribution they may lead tonarrower conﬁdence intervals, thus artiﬁcially increasing thestatistical power. On the other hand, duplication of “deviant”cases reduces the power of estimates. In both cases the e ectis small, up to 3% of the true standard errors. Yet, the moste ective solutions to reduce the bias of coe cients, i.e. drop-ping the superﬂuous duplicates and weighting by the inverseof multiplicity, increase the estimated standard errors, thusreducing the power of estimates.3.3Risk of obtaining biased estimatesWhile percentage bias informs about the average bias dueto duplicates, it is plausible that estimates in speciﬁc repli-cations have upward and downward biases which, on aver-age, o set each other. In other words, even with moderatebias, researchers can obtain biased estimates in speciﬁc es-timations. To address this issue we turn to the analysis ofDfbetas.Figure 3 shows box and whiskers diagrams of Dfbetas inScenarios 2 (upper panel) and 3 (lower panel) for Variant i,i.e. when the duplicates are randomly drawn from the over-all distribution of the dependent variable. We do not report24FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable 2Percentage bias of thex coecient (as a percentage ofx).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decupletScenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doubletsScenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets0:00:00:00:30:30:50:30:40:60:20:30:30:00:00:00:82:14:21:02:34:50:61:52:90:10:00:01:43:36:31:53:46:21:12:34:30:00:00:00:10:10:10:10:00:00:00:00:00:00:00:10:82:14:21:02:65:60:71:84:00:10:10:00:40:81:60:41:02:00:30:71:30:00:00:00:10:10:10:10:00:00:00:00:00:00:00:00:72:04:12:87:114:42:15:611:40:10:00:01:23:16:12:25:410:81:74:48:80:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:20:00:00:30:00:00:10:00:00:00:00:10:10:00:00:20:00:00:20:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:20:00:00:30:00:00:10:00:00:00:00:10:10:00:00:20:00:00:2BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONSTable 3Average standard error of x coe cient (expressed as a percentage of the true stan-dard error of x).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) Weightedregression25Scenario 1Variant i: “unconstrained”1 sextuplet 100:01 octuplet 100:01 decuplet 100:0Variant ii: “typical”1 sextuplet 99:91 octuplet 99:81 decuplet 99:8Variant iii: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:2Variant iv: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:2Scenario 2Variant i: “unconstrained”16 doublets 100:040 doublets 100:079 doublets 100:0Variant ii: “typical”16 doublets 99:640 doublets 99:079 doublets 98:0Variant iii: “deviant”16 doublets 100:440 doublets 100:979 doublets 101:7Variant iv: “deviant”16 doublets 100:440 doublets 100:979 doublets 101:8Scenario 3Variant i: “unconstrained”7 quintuplets 100:016 quintuplets 100:031 quintuplets 100:0Variant ii: “typical”7 quintuplets 99:316 quintuplets 98:531 quintuplets 97:0Variant iii: “deviant”7 quintuplets 100:616 quintuplets 101:331 quintuplets 102:4Variant iv: “deviant”7 quintuplets 100:716 quintuplets 101:431 quintuplets 102:5100:2100:3100:3100:2100:3100:4100:2100:2100:3100:2100:2100:3101:1102:8105:7101:5103:8107:8100:7101:8103:5100:7101:7103:3101:2102:8105:6101:4103:2106:4101:0102:4104:8101:0102:4104:8100:0100:0100:0100:1100:1100:1100:0100:0100:0100:0100:0100:0100:0100:0100:099:799:198:199:197:695:199:097:394:4100:0100:0100:099:598:697:299:398:396:599:298:096:0100:2100:2100:3100:2100:2100:3100:2100:2100:3100:2100:2100:3100:5101:4102:7100:5101:3102:6100:5101:4102:9100:6101:4102:9100:9102:2104:4100:9102:2104:4101:0102:2104:5101:0102:2104:599:9100:0100:099:9100:0100:199:9100:0100:099:9100:0100:0100:0100:5101:1100:2100:9102:099:9100:0100:399:999:9100:1100:6101:5103:2100:6101:8103:8100:4101:2102:8100:4101:2102:626FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKAresults for Scenario 1 because in this case the risk of obtain-ing biased estimates is virtually zero. Results, however, aredetailed in Table 4. On the y-axis we report the Dfbetas, onthe x-axis we report the coe cients and the solutions. Thetwo horizontal solid lines identify the cuto values of Dfbe-tas (0.5) separating the replications with acceptable bias fromthe unacceptable ones. The diagrams show that the range ofDfbetas increases with the share of duplicates in the data,and it is larger for quintuplets (Scenario 3) than for doublets(Scenario 2).The average probability of obtaining unbiased estimatesfor all coe cients is shown in Table 4. Column a shows that,in case of “naive” estimations, the risk of biased estimatesvaries from 0,14% (Scenario 1, one sextuplet, Variant ii) toabout 58% (Scenario 3, 31 quintuplets, Variants iii and iv).In Scenario 1 the risk is small, with 89%–99% probability ofobtaining unbiased estimates.The results show three regularities. First, the risk of ob-taining biased estimates increases with the share of dupli-cates: it is 0.2%–0.7% (depending on the variant) for 16doublets, but it grows to 14.0%–32.5% when 79 doubletsare included in the data. In Scenario 3 it grows from 3.0%–20.2% for 7 quintuplets, to 43.0%–58.6% when data contain31 quintuplets.Second, when the duplicates constitute the same share ofthe data, the risk of obtaining biased estimates is higher forquintuplets than for doublets. For example, when duplicatesconstitute 2% of the sample, the risk of obtaining biased es-timates is below 1% if they are 16 doublets, but ranges be-tween 3% and 20% (depending on the variant) for 7 quintu-plets. When duplicates constitute 10% of the data, the riskof obtaining biased estimates is 14%–32% in case of 79 dou-blets, but 43%–58% for 31 quintuplets.Third, the risk of obtaining biased estimates is the highestin Variants iii and iv, i.e. when the duplicates are locatedon the ties, and lowest in Variant ii, when the duplicates arelocated around the median. For example, with 7 quintuplets,the probability of obtaining biased estimates is about 3% inVariant ii, but rises to about 20% in Variants iii and iv. For 31quintuplets the risk is about 43% in Variant ii, but over 58%in Variants iii and iv.As in case of percentage bias, weighting by the inverseof the multiplicity (Solution e), and dropping the superﬂuousduplicates (Solution d) perform better than other solutions(see Table 4 and Figure 3). In Scenario 2, when doublets con-stitute about 5% of the data, these two solutions reduce theprobability of obtaining biased estimates from about 4% (inVariants i and ii) or about 11% (Variants iii and iv) to under1% in all cases. In case of 79 doublets, i.e when duplicatesconstitute about 10% of the sample, the risk of obtaining bi-ased estimates reduces to about 3%, independently from thelocation of the duplicates, whereas it ranges between 14%and 33% for the naive estimation. In Scenario 3, when quin-tuplets constitute about 5% of the data, the risk of obtainingbiased estimates declines from 19%–43% (depending on thevariant) to about 2%. When quintuplets constitute about 10%of the sample, solutions d and e decrease the risk of obtainingbiased estimates from 43%–59% to about 9%.To sum up, weighting by the inverse of multiplicity anddropping the superﬂuous duplicates are the most e ective so-lutions among the examined ones. Moreover, they performparticularly well when the duplicates are located on the tiesof the distribution, i.e. when the risk of bias is the highest.On the other hand, Solutions b (excluding all duplicates)and c (ﬂagging the duplicates) perform worse. Flagging du-plicates and controlling for them (c) fails to reduce the riskof obtaining biased estimates in both Scenarios 2 and 3. Ex-cluding all duplicates (b) reduces the risk of obtaining biasedestimates in Scenario 3 (quintuplets), but it performs poorlyin Scenario 2: if the doublets are located on the ties, thendropping all duplicate records decreases the probability ofobtaining unbiased estimates.3.4Root Mean Square ErrorTable 5 shows the values of normalized RMSE for coe -cient x. Results for other coe cients are available in TablesD1–D3 in Appendix D. Scores are overall small, reachingabout 9% of the x coe cient, 15% of z, 24% of t, and 6%of the intercept.The RMSE captures both the bias and the standard errors,thus it is not surprising that the results are consistent withthose presented in the sections above. First, RMSE increaseswith the number of duplicates, and it is the highest for 79doublets and 31 quintuplets. Second, the presence of ran-domly duplicated observations (Variant i) has little e ect onthe e ciency of the estimates, whereas the presence of “typ-ical” (Variant ii) and “deviant” (Variant iii and iv) duplicatesreduces the e ciency of estimates.Consistently with previous results, Solutions d and e, i.e.dropping superﬂuous duplicates and weighting the data per-form the best, reasonably reducing the RMSE values. In con-trast to that, ﬂagging the duplicates and controlling for them(Solution c) performs poorly, and in some cases (especiallyin Scenario 2, but for x also in Scenario 3) it further reducesthe e ciency of the estimates.3.5RobustnessVarying sample size. By setting up our experiment, wearbitrarily chose a sample size of N = 1; 500 observationsto mimic the average size of many of the publicly availablesocial surveys. To check whether our results are independentfrom our choice, we repeated the experiment using two al-ternative samples: N = 500 and N = 5; 000. In Figure 4we report the results (DFbetas) for Scenario 2, Variant i. Thecomplete set of results is available upon request.211−2−422−stea 0b−fdsateb−fd 0a b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x z7 quintuplets (2%)16 quintuplets (5%)31 quintuplets (10%)BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS2716 doublets (2%)40 doublets (5%)79 doublets (10%)a b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x zFigure 3. Box and whiskers diagrams of Dfbetas in Scenario 2 and 3, Variant i. The duplicate records are randomly drawnfrom the overall distribution. Box and whiskers show the distribution of Dfbetas (across 2,500 replications) for each of thecoe cients in the model and for the solutions a to e.Notes: a: “Naive” estimation; b: Drop all duplicates; c: Flag and control; d: Drop superﬂuous duplicates; e: Weightedregression. _cons: regression constant; x: x; z: z; t: t.28FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable 4Probability of obtaining unbiased estimates (Dfbetai < 0:5).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decuplet99:297:094:499:999:798:997:994:790:998:194:589:2Scenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doublets99:896:586:0100:096:377:199:388:566:599:389:267:7Scenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets89:271:955:797:080:757:180:358:241:479:957:141:9100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:099:896:486:9100:096:880:599:286:860:799:087:161:299:796:487:399:896:887:899:693:981:599:794:181:7100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:099:896:586:1100:096:678:096:668:344:197:463:725:991:072:456:498:082:358:595:372:248:595:272:444:2100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:099:696:1100:099:696:1100:099:696:8100:099:796:899:998:091:299:997:790:6100:097:890:7100:097:991:4100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:0100:099:796:4100:099:796:5100:099:796:9100:099:796:999:998:291:899:998:191:4100:098:191:0100:098:191:7BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS29Table 5Normalized RMSE of thex coecient (in percentage).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decupletScenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doubletsScenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets9:19:19:19:19:19:19:19:19:19:19:19:19:19:19:19:19:29:89:29:510:39:29:39:79:19:19:19:29:610:99:39:811:29:29:510:39:19:19:19:19:19:19:19:19:19:19:19:19:29:49:69:39:710:79:29:610:99:29:410:29:29:39:69:29:49:89:29:49:79:29:39:69:19:19:19:19:19:19:19:19:19:19:19:19:19:19:19:19:29:89:411:416:89:310:514:39:19:19:19:19:510:89:310:513:99:29:912:49:19:19:19:19:19:19:19:19:19:19:19:19:19:29:39:19:29:39:19:29:49:19:29:49:29:39:59:29:39:59:29:39:59:29:39:59:19:19:19:19:19:19:19:19:19:19:19:19:19:19:29:19:29:39:19:19:19:19:19:19:19:29:49:29:39:49:19:29:49:19:29:3stea 0b−fdstea 0b−fd211−2−211−2−a b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x z50 doublets (2%)130 doublets (5%)270 doublets (10%)30FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKA5 doublets (2%)13 doublets (5%)27 doublets (10%)a b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x za b c d e a b c d e a b c d e a b c d e_cons t x zFigure 4. Box and whiskers diagrams of Dfbetas in Scenario 2, Variant i, for N = 500 and N = 5; 000. The duplicaterecords are randomly drawn from the overall distribution. Box and whiskers show the distribution of Dfbetas (across 2,500replications) for each of the coe cients in the model and for the solutions a to e.Notes: a: “Naive” estimation; b: Drop all duplicates; c: Flag and control; d: Drop superﬂuous duplicates; e: Weightedregression. _cons: regression constant; x: x; z: z; t: t.BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS31Figure 4 shows that the dispersion of the estimates withrespect to the true values increases when the number of dupli-cates increases. Neglecting the presence of duplicates createssome problems when the share of duplicates reaches about5% of the sample.For N = 500 the probabilities of obtaining biased esti-mates amount to 2% and 11.4% when the doublets constitute5% and 10% of the sample respectively. For N = 5; 000 thesame probabilities are 3% and 13%. These values are fairlyin line with the results obtained for N = 1; 500 for Variant i(3.5% and 14%).Consistently with the results for N = 1; 500, weightingby the inverse of the multiplicity or dropping all superﬂuousduplicates are most e ective in reducing the risk of obtainingbiased estimates. Our conclusion about the inﬂuence of du-plicated records and the e ciency of the solutions does notdepend on sample size.Typical and deviant cases deﬁned on the basis of thedistribution of the x variable. To check the robustness ofour ﬁndings, we follow the same scheme to analyze how theposition of the duplicates on the distribution of the indepen-dent variable x (rather than the dependent variable y) a ectsregression estimates. Results are consistent with those pre-sented above, and are available upon request.4ConclusionsReliable data are a prerequisite for well grounded analy-ses. In this paper we focused on the consequences of du-plicate records for regression estimates. A review of theliterature shows that there are no papers dealing with thistopic. Yet, two recent independent studies by Slomczynskiet al. (2017) and by Kuriakose and Robbins (2016) raised theawareness about the quality of survey data and they warnedabout the possible consequences of ignoring the presence ofduplicate records. The two teams of researchers showed thata number of widely used surveys is a ected by duplicaterecords to varying degrees. Unfortunately, little is knownabout the bias and e ciency loss induced by duplicates insurvey data. Present paper partly ﬁlls this gap by address-ing two research questions: ﬁrst, how do duplicates a ectregression estimates? Second, how e ective are the possiblesolutions to deal with duplicates?To this aim we created an artiﬁcial data set of N = 1; 500observations and four variables with a known covariance ma-trix. We adopted a Monte Carlo simulation with 2; 500 repli-cations to investigate the consequences of 36 patterns (3 sce-narios 3 cases in each scenario 4 variants) of duplicaterecords. The scenarios included: (1) multiple duplications ofa single record: sextuplet, octuplet and decuplet; (2) multi-ple doublets (16, 40, 79, corresponding to 2%, 5%, and 10%of the sample); and (3) multiple quintuplets (7, 16, 31, cor-responding to 2%, 5%, and 10% of the sample). The fourvariants allowed us to investigate whether the reliability ofregression estimates changed when the duplicates were situ-ated in speciﬁc parts of the data distribution: (i) on the wholedistribution, (ii) around the median, (iii) on the lower tie, and(iv) on the upper tie of the distribution of the dependent vari-able.For each of the scenarios we run a “naive” estimation,which ignored the presence of duplicate records. This al-lowed us to investigate the consequences of duplicate recordsfor regression estimates. Speciﬁcally, we investigated thepercentage bias, the standard errors, the risk of obtaining bi-ased estimates, and the root mean square error (RMSE) tounderstand under which conditions, and to which extent thepresence of duplicates is problematic.The results showed that duplicates may bias regression es-timates when duplicate records are located in speciﬁc partsof the distribution. In other words, the bias was null whenthe duplicates were randomly drawn from the overall distri-bution of the dependent variable (Variant i). Interestingly,duplicating “typical” cases (Variant ii) was just as problem-atic as duplicating “deviant” cases (Variants iii and iv). Inour simulation the bias was rather low: it reached the highestvalue of about 7% when the data contained 31 quintuplets.Overall, the bias increased with the share of duplicates in thedata, and it was higher for quintuplets than for doublets.The presence of duplicates in the data a ected also thestandard errors, and therefore the conﬁdence intervals. Sim-ilarly as in the case of the percentage bias, duplicates ran-domly chosen from the overall distribution (Variant i) did nota ect standard errors. Duplicating “typical” cases (Variantii) biased the standard errors downwards, thus increasing thestatistical power of the estimates. On the contrary, the pres-ence of “deviant” cases (Variants iii and iv) biased the stan-dard errors upwards, thus producing less precise estimates.The bias of standard errors was overall low (up to maximum3%), and it was higher when more duplicates were present inthe data.The presence of duplicates also a ected the risk of ob-taining biased estimates. We considered as biased the co-e cients that departed by at least 0.5 standard errors fromthe true value. The risk of obtaining biased estimates in-creased with the share of duplicates in the data, reaching thevalues between 44%–59% (depending on the Variant) when31 quintuplets were present in the data. The risk was alsohigher when the duplicates were located on the ties of thedistribution (Variants iii and iv), and it was the lowest whenthe duplicates were located in the center of the distribution(Variant ii). Also the pattern of duplicates mattered, withquintuplets being more problematic than doublets.The above results are interesting in the light of previousstudies which discussed the possible consequences of dupli-cates for regression estimates. We found no evidence of at-tenuation bias, which suggests that duplicates do not intro-duce random noise in the data. Moreover, we did not ﬁnd any32FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKAbias when duplicates were located randomly on the overalldistribution. On the other hand, if the duplicates were lo-cated in a speciﬁc part of the distribution, the bias was sys-tematic. Moreover, we found that duplicates increased thestatistical power of estimates if the duplicated cases were lo-cated in the center of the distribution. On the contrary, whenduplicates were located on the ties of the distribution, theybiased the conﬁdence intervals upwards. We also found thatduplication of “typical” cases is as problematic as duplica-tion of “deviant” cases. It may be even considered moreproblematic because the bias produced by “typical” dupli-cates is accompanied by narrower conﬁdence intervals, i.e.higher statistical signiﬁcance. On the other hand, biased co-e cients produced by “deviant” duplicates are accompaniedby broader conﬁdence intervals.The number and patterns of duplicate records used in thisanalysis are consistent with those identiﬁed by Slomczynskiet al. (2017), and they can, therefore, be regarded as realistic.Hence, our ﬁrst conclusion is that although the bias and e -ciency loss related to duplicate records are small, duplicatescreate a risk of obtaining biased estimates. Thus, researcherswho use data with duplicate records risk to reach misleadingconclusions.The second goal of our analysis was to investigate the ef-ﬁcacy of four solutions to reduce the e ect of duplicates onestimation results. They included: (b) dropping all duplicatesfrom the sample; (c) ﬂagging duplicates and controlling forthem in the estimation; (d) dropping all superﬂuous dupli-cates; (e) weighting the observations by the inverse of theduplicates’ multiplicity.The techniques that performed the best are solutions d ande, which basically reduced the bias of the coe cient to zero.They also performed well in reducing the risk of obtainingbiased estimates. The downside is that these solutions bi-ased upwards the estimated standard errors. Hence, althoughdropping the superﬂuous duplicates or weighting the obser-vations by the inverse of the duplicates’ multiplicity allow toobtain unbiased coe cients, these solutions come at the costof decreasing the statistical power of estimates.The solution which performed the worst was ﬂagging du-plicates and controlling for them in the estimation. It pro-duced coe cients’ estimates that were more biased thanthose obtained in the naive estimation. Additionally, it sys-tematically underestimated the standard errors. This is a par-ticularly worrisome combination because biased coe cientswere associated to a higher statistical conﬁdence.Hence, the second conclusion from our study is thatweighting the duplicates by the inverse of their multiplic-ity or dropping the superﬂuous duplicates are the best so-lutions among the considered ones. These solutions outper-form all the others in reducing the percentage bias, in reduc-ing the risk of obtaining biased estimates, and minimizing theRMSE. Unfortunately, they are associated to larger standarderrors, and therefore to lower statistical power. Flagging du-plicates and controlling for them is consistently a worst solu-tion, and in some cases (especially for Scenario 2) it producesa higher bias, narrower conﬁdence intervals, higher risk ofobtaining biased estimates, and greater e ciency loss thanthe “naive” estimation.Our results do not depend on the sample size we chose(N = 1; 500): they do not change whether we use a smaller(N = 500) or a larger (N = 5; 000) sample. Similarly, theresults do not change if the variants are deﬁned on the basisof one of the independent variables in the regression ratherthan the dependent one.These are the ﬁrst results documenting the e ect of dupli-cates for survey research, and they pave the road for furtherresearch on the topic. For instance, our study considered anideal case in which the model used by researchers perfectlyﬁtted the relationship in the data, i.e. all relevant predic-tors were included in the model. This is an unusual situa-tion in social research. Second, our study did not accountfor heterogeneity of populations. We analyze a case whenthe relationships of interest are the same for all respondents.In other words, we considered a situation without unmod-eled interactions among variables. Third, in our model therecords which were substituted by duplicates (the interviewswhich would have been conducted if no duplicates were in-troduced in the data) were selected randomly. In reality thisis probably not the case, as these are likely the respondentswho are the most di cult to reach by interviewers. Plausi-bly, the omitted variables, the heterogeneity of the popula-tion, and the non-random choice of the interviews replacedby the duplicates exacerbate the impact of duplicates on re-gression coe cients. This suggests that our estimates of thee ect of duplicates on percentage bias, standard errors, riskof obtaining biased estimates, and e ciency loss are in manyaspects conservative. Moreover, our study assumed that non-unique records were duplicates of true interviews and notpurposefully generated fakes. Addressing these limitationsis a promising path for future research.Overall, our results emphasize the importance of collect-ing data of high quality, because correcting the data withstatistical tools is not a trivial task. This calls for furtherresearch about how to address the presence of duplicates inthe data and for more reﬁned statistical tools to minimize theconsequent bias of coe cients and standard errors, the riskof obtaining biased estimates, and the e ciency loss.AcknowledgementsThe authors wish to thank Kazimierz M. Slomczynski,Przemek Powałko, and the participants to the HarmonizationProject of the Polish Academy of Science for their commentsand suggestions. Possible errors or omissions are entirely theresponsibility of the authors who contributed equally to thiswork.BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS33ReferencesAmerican Statistical Association. (2004). Interviewer falsi-ﬁcation in survey research: Current best methods forprevention, detection, and repair of its e ects. SurveyResearch. Newsletter from the Survey Research Labo-ratory, College of Urban Planning and Public A airs,University of Illinois at Chicago. 35(1), 1–5.Diekmann, A. (2005). Betrug und Täuschung in der Wis-senschaft. Datenfälschung, Diagnoseverfahren, Kon-<ref>sequenzen. Schweizerische Zeitschrift für Soziologie,</ref>31(1), 7–29.<ref>Elmagarmid, A. K., Ipeirotis, P. G., & Verykios, V. S. (2007).</ref>Duplicate record detection: a survey. IEEE Transac-tions on Knowledge and Data Engineering, 19(1), 1–16.European Social Survey. (2015). European Social Surveyround 6. Bergen: Norwegian Social Science Data Ser-vices.<ref><ref>Ferrarini, A. (2011). A ﬁtter use of Monte Carlo simula-</ref>tions in regression models. Computational Ecologyand Software, 1(4), 240–243.Finn, A. & Ranchhod, V. (2013). Genuine fakes: The preva-lence and implications of ﬁ</ref>e Carlo simula-</ref>tions in regression models. Computational Ecologyand Software, 1(4), 240–243.Finn, A. & Ranchhod, V. (2013). Genuine fakes: The preva-lence and implications of ﬁeldworker fraud in a largeSouth African survey. Working Paper 115 of the South-ern Africa Labour and Development Research Unit,University of Cape Town, 115.<ref>Fishman, G. (2005). A ﬁrst course in Monte Carlo. Duxbury</ref>Press.<ref>Hassanzadeh, O. & Miller, R. J. (2009). Creating prob-</ref>abilistic databases from duplicated data. The VLDBJournal–The International Journal on Very LargeData Bases, 18(5), 1141–1166.<ref>Hill, T. P. (1999). The di culty of faking data. Chance,</ref>12(3), 27–31.Koczela, S., Furlong, C., McCarthy, J., & Mushtaq, A.(2015). Curbstoning and beyond: confronting data fab-<ref>rication in survey research. Statistical Journal of the</ref>IAOS, 31(3), 413–422.Kuriakose, N. & Robbins, M. (2016). Don’t get duped: Fraud<ref>through duplication in public opinion surveys. Statisti-</ref>cal Journal of the IAOS, 32(3), 283–291.<ref>Lessler, J. & Kalsbeek, W. (1992). Nonsampling error in sur-</ref>veys. New York: Wiley.<ref>Schnell, R. (1991). Der Einﬂuß gefälschter Interviews auf</ref>Survey-Ergebnisse. Zeitschrift für Soziologie, 20(1),25–35.<ref></ref> G. G. (2005). Characteristics</ref>and impact of faked interviews in surveys–An analysisof genuine fakes in the raw data of SOEP. AllgemeinesStatistisches Archiv, 89</ref</ref>. (2005). Characteristics</ref>and impact of faked interviews in surveys–An analysisof genuine fakes in the raw data of SOEP. AllgemeinesStatistisches Archiv, 89</ref>Characteristics</r</ref>05). Characteristics</ref>and impact of faked interviews in surveys–An analysisof genuine fakes in the raw data of SOEP. AllgemeinesStatistisches Archiv, 89</ref>Characteristics</ref>and impact of faked interviews in surveys–An analysisof genuine fakes in the raw data of SOEP. AllgemeinesStatistisches Archiv, 89(1), 7–20.Schreiner, I., Pennie, K., & Newbrough, J. (1988). Inter-viewer falsiﬁcation in Census Bureau surveys. In Pro-ceedings of the<ref></ref> American Statistical Association (Sur-vey Research Methods Section) (pp. 491–496).Slomczynski, K. M., Powałko, P., & Krauze, T. (2017). Non-unique records in International Survey Projects: Theneed for extending data quality control. Survey Re-search Methods, 11(1), 1–16. doi:doi:10.18148/srm/2017.v11i1.6557Waller, L. G. (2013). Interviewing the surveyors: Factorswhich contribute to questionnaire falsiﬁcation (curb-stoning) among Jamaican ﬁeld surveyors. Interna-tional Journal of Social Research Methodology, 16(2),155–164.34FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKAAppendix ADescriptive statistics for the simulated data sets.(see table A1 below)Appendix BPercentage bias for the remaining coe cients(see tables B1–B3 below)Appendix CStandard errors for the remaining coe cients(see tables C1–C3 below)Appendix DRoot mean square error for the remaining coe cients(see tables D1–D3 below)BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONSTable A1Descriptive statistics for the initial data set and for exemplary simulated data sets.35N. of duplicatesVariablessdminmaxmissingInitial data set0000Scenario 11 sextuplet1 sextuplet1 sextuplet1 sextuplet1 sextuplet1 octuplet1 octuplet1 octuplet1 octuplet1 octuplet1 decuplet1 decuplet1 decuplet1 decuplet1 decupletScenario 216 doublets16 doublets16 doublets16 doublets16 doublets40 doublets40 doublets40 doublets40 doublets40 doublets79 doublets79 doublets79 doublets79 doublets79 doubletsScenario 37 quintuplets7 quintuplets7 quintuplets7 quintuplets7 quintuplets16 quintuplets16 quintuplets16 quintuplets16 quintuplets16 quintuplets31 quintuplets31 quintuplets31 quintuplets31 quintuplets31 quintupletsyxztyxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)yxztduplicates (ﬂag)mean5:21348:044:91640:032:58816:862:40213:355:212 2:58747:99 16:834:911 2:40040:01 13:330:00400 0:06315:225 2:58847:96 16:894:930 2:40339:90 13:430:00533 0:07295:187 2:59547:93 16:874:909 2:39339:98 13:280:00667 0:08145:217 2:58248:06 16:924:933 2:40940:00 13:320:0213 0:1455:219 2:59948:18 16:814:929 2:41039:94 13:440:0533 0:2255:22747:994:89640:010:1052:58216:942:40413:420:3075:219 2:58448:09 16:874:932 2:40439:81 13:470:0233 0:1515:240 2:63148:08 16:714:918 2:45339:91 13:440:0533 0:2255:19848:154:94139:710:1032:59816:612:45813:390:3043:878 14:0214:13 99:643:839 13:995:743 90:413:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 99:643:839 13:995:743 90:410 13:878 14:0214:13 97:583:839 13:995:743 90:410 1obs1500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500150015001500000000000000000000000000000000000000000000000000036FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable B1Percentage bias of thez coecient (expressed as a percentage of z)Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decupletScenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doubletsScenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets0:00:00:10:30:30:50:30:20:50:30:40:60:00:00:00:92:34:70:71:93:51:12:75:30:20:10:21:53:66:81:22:64:82:04:57:00:00:00:00:10:00:00:00:10:00:00:10:00:10:00:00:92:34:60:71:94:11:12:86:00:10:20:10:40:81:90:30:81:50:51:22:10:00:00:00:10:00:00:00:10:00:00:10:00:00:00:00:82:24:61:54:08:22:15:010:30:10:10:21:23:36:61:12:95:51:33:26:60:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:30:00:20:20:00:20:50:10:20:10:00:10:00:00:10:10:10:00:10:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:30:00:20:20:00:20:50:10:20:10:00:10:00:00:10:10:10:00:1BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS37Table B2Percentage bias of the t coecient (expressed as a percentage of t)Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decupletScenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doubletsScenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets0:10:00:10:20:30:60:20:30:80:40:40:40:10:20:20:92:44:80:91:93:70:92:65:00:00:10:41:63:87:21:42:95:01:74:37:30:00:00:00:10:10:10:00:00:10:00:00:10:00:10:21:02:24:40:72:24:61:03:16:00:00:00:00:31:01:60:30:81:60:41:12:30:00:00:00:10:10:10:00:00:10:00:00:10:10:20:10:82:34:71:95:210:72:15:811:00:00:10:31:33:67:01:53:67:61:63:77:60:00:00:00:10:00:00:00:00:00:10:00:00:00:10:20:00:20:40:10:00:30:00:00:30:00:00:10:10:00:30:10:00:10:00:10:10:00:00:00:10:00:00:00:00:00:10:00:00:00:10:20:00:20:40:10:00:30:00:00:30:00:00:10:10:00:30:10:00:10:00:10:138FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable B3Percentage bias of the intercept (expressed as a percentage of the intercept)Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decupletScenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doubletsScenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets0:00:00:00:00:00:00:10:20:30:20:20:30:00:00:00:00:00:10:51:22:50:51:32:50:00:00:10:10:10:20:91:93:80:91:94:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:00:10:20:51:22:40:51:32:70:00:00:00:00:00:10:20:50:90:20:51:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:30:70:82:04:10:00:00:10:00:10:10:20:40:80:51:22:20:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:10:00:00:10:00:00:00:00:00:00:00:00:10:00:00:10:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:10:10:00:00:10:00:00:00:00:00:00:00:00:10:00:00:1BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONSTable C1Average standard error of z coe cient (expressed as a percentage of the true stan-dard error of z).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) Weightedregression39Scenario 1Variant i: “unconstrained”1 sextuplet 100:01 octuplet 100:01 decuplet 100:0Variant ii: “typical”1 sextuplet 99:91 octuplet 99:81 decuplet 99:8Variant iii: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:2Variant iv: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:3Scenario 2Variant i: “unconstrained”16 doublets 100:040 doublets 100:079 doublets 100:0Variant ii: “typical”16 doublets 99:640 doublets 98:979 doublets 97:7Variant iii: “deviant”16 doublets 100:440 doublets 101:079 doublets 101:8Variant iv: “deviant”16 doublets 100:440 doublets 101:179 doublets 102:1Scenario 3Variant i: “unconstrained”7 quintuplets 100:016 quintuplets 100:031 quintuplets 100:0Variant ii: “typical”7 quintuplets 99:316 quintuplets 98:331 quintuplets 96:6Variant iii: “deviant”7 quintuplets 100:716 quintuplets 101:531 quintuplets 102:7Variant iv: “deviant”7 quintuplets 100:816 quintuplets 101:731 quintuplets 103:0100:2100:3100:3100:2100:3100:4100:2100:2100:3100:2100:2100:3101:1102:8105:7101:5103:9108:1100:7101:7103:3100:6101:5103:0101:2102:8105:6101:4103:2106:5101:0102:4104:7101:0102:3104:7100:0100:0100:0100:1100:1100:1100:0100:0100:0100:0100:0100:0100:0100:0100:099:799:097:899:097:394:499:097:394:4100:0100:0100:099:498:496:899:298:196:199:398:196:2100:2100:2100:3100:2100:2100:3100:2100:2100:3100:2100:2100:3100:5101:4102:7100:5101:3102:6100:5101:4102:9100:6101:4102:9100:9102:2104:4100:9102:2104:3101:0102:2104:5101:0102:2104:5101:9102:0102:0101:9102:0102:1101:9101:9102:0101:9101:9102:0102:0102:4103:1102:2102:9104:0101:9102:0102:3101:8101:9102:0102:5103:5105:3102:7103:8105:9102:4103:3104:7102:4103:2104:640FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable C2Average standard error of t coe cient (expressed as a percentage of the true stan-dard error of t).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet 100:01 octuplet 100:01 decuplet 100:0Variant ii: “typical”1 sextuplet 99:91 octuplet 99:81 decuplet 99:8Variant iii: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:2Variant iv: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:2Scenario 2Variant i: “unconstrained”16 doublets 100:040 doublets 100:079 doublets 100:0Variant ii: “typical”16 doublets 99:640 doublets 98:979 doublets 97:8Variant iii: “deviant”16 doublets 100:440 doublets 101:079 doublets 101:9Variant iv: “deviant”16 doublets 100:440 doublets 101:079 doublets 101:9Scenario 3Variant i: “unconstrained”7 quintuplets 100:016 quintuplets 100:031 quintuplets 100:0Variant ii: “typical”7 quintuplets 99:316 quintuplets 98:331 quintuplets 96:7Variant iii: “deviant”7 quintuplets 100:716 quintuplets 101:531 quintuplets 102:7Variant iv: “deviant”7 quintuplets 100:716 quintuplets 101:531 quintuplets 102:6100:2100:3100:3100:2100:3100:4100:2100:2100:3100:2100:2100:3101:1102:8105:8101:5103:9108:0100:6101:6103:2100:7101:6103:2101:2102:8105:6101:4103:2106:5101:0102:3104:7101:0102:4104:7100:0100:0100:0100:1100:1100:1100:0100:0100:0100:0100:0100:0100:0100:0100:199:799:097:999:097:394:598:997:294:1100:0100:0100:199:498:496:999:398:196:199:297:995:7100:2100:2100:3100:2100:2100:3100:2100:2100:3100:2100:2100:3100:5101:4102:8100:5101:3102:6100:5101:4102:9100:5101:4102:9101:0102:2104:4100:9102:2104:4101:0102:2104:5101:0102:2104:5106:6106:6106:7106:6106:7106:7106:5106:6106:7106:5106:6106:7106:7107:1107:8106:9107:6108:8106:5106:5106:8106:5106:6106:8107:2108:2110:1107:4108:6110:7107:1107:9109:4107:1108:0109:5BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONSTable C3Average standard error of the intercept (expressed as a percentage of the true stan-dard error of the intercept).Solution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) Weightedregression41Variant i: “unconstrained”1 sextuplet 100:01 octuplet 100:01 decuplet 100:0Variant ii: “typical”1 sextuplet 99:91 octuplet 99:81 decuplet 99:8Variant iii: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:3Variant iv: “deviant”1 sextuplet 100:11 octuplet 100:21 decuplet 100:2Scenario 2Variant i: “unconstrained”16 doublets 100:040 doublets 100:079 doublets 100:0Variant ii: “typical”16 doublets 99:640 doublets 98:979 doublets 97:8Variant iii: “deviant”16 doublets 100:540 doublets 101:179 doublets 102:1Variant iv: “deviant”16 doublets 100:440 doublets 101:079 doublets 101:8Scenario 3Variant i: “unconstrained”7 quintuplets 100:016 quintuplets 100:031 quintuplets 100:0Variant ii: “typical”7 quintuplets 99:316 quintuplets 98:331 quintuplets 96:7Variant iii: “deviant”7 quintuplets 100:816 quintuplets 101:731 quintuplets 103:0Variant iv: “deviant”7 quintuplets 100:716 quintuplets 101:531 quintuplets 102:5100:2100:3100:3100:2100:3100:4100:2100:2100:3100:2100:2100:3101:1102:8105:8101:5103:9108:0100:6101:5103:0100:7101:7103:3101:2102:8105:6101:4103:2106:5101:0102:3104:6101:0102:4104:8100:0100:0100:0100:1100:1100:1100:0100:0100:0100:0100:0100:0100:1100:2100:399:799:298:299:097:494:699:097:294:3100:0100:1100:399:598:697:199:398:296:399:297:995:8100:2100:2100:3100:2100:2100:3100:2100:2100:3100:2100:2100:3100:5101:4102:8100:5101:3102:6100:5101:4102:9100:5101:4102:9100:9102:2104:4100:9102:2104:4101:0102:2104:5101:0102:2104:5103:9104:0104:1104:0104:0104:1103:9104:0104:0103:9104:0104:0104:1104:5105:2104:3105:0106:1103:9104:0104:3103:9104:0104:1104:6105:6107:4104:7105:9108:0104:4105:3106:8104:4105:2106:742FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable D1Normalized RMSE for thez coecientSolution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decuplet14:814:814:814:714:714:714:814:814:814:814:814:8Scenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doublets14:814:814:814:714:815:214:815:015:414:915:216:0Scenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets14:814:814:814:714:915:814:915:215:915:015:716:714:814:814:814:814:814:814:814:814:814:814:814:814:915:215:615:015:516:614:915:115:814:915:216:314:915:215:615:015:315:814:915:115:514:915:215:614:814:814:814:814:814:814:814:814:814:814:814:814:814:814:814:714:815:214:714:916:214:815:217:314:814:814:814:714:915:714:714:815:214:714:815:714:814:814:814:814:814:814:814:814:814:814:814:814:815:015:214:815:015:214:815:015:214:815:015:214:915:115:414:915:115:414:915:115:414:915:115:415:015:115:115:015:115:115:015:115:115:015:115:115:115:115:215:115:215:415:015:115:115:015:015:115:115:315:515:215:315:615:115:215:515:115:215:4BIAS AND EFFICIENCY LOSS IN REGRESSION ESTIMATES DUE TO DUPLICATED OBSERVATIONS43Table D2Normalized RMSE for the t coecientSolution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionVariant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decuplet24:324:324:324:324:324:224:324:324:424:324:324:4Scenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doublets24:324:324:324:224:224:224:424:625:024:424:725:3Scenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets24:324:324:324:224:224:624:524:825:524:525:026:024:324:424:424:424:424:424:324:424:424:324:424:424:625:025:724:725:326:624:524:825:524:524:925:824:625:025:724:625:125:924:524:925:524:524:925:524:324:324:324:324:324:324:324:324:324:324:324:324:324:324:324:224:224:224:124:225:324:124:325:424:324:324:324:224:224:524:224:124:624:224:124:524:324:424:424:324:424:424:324:424:424:324:424:424:424:625:024:424:624:924:424:625:024:424:625:024:524:825:424:524:825:424:524:825:424:524:825:425:925:925:925:925:925:925:925:925:925:925:925:925:926:026:226:026:126:425:925:925:925:925:926:026:126:326:826:126:426:926:026:226:626:026:226:644FRANCESCO SARRACINO AND MAŁGORZATA MIKUCKATable D3Normalized RMSE for the interceptSolution(a) “Naive”estimation(b) Dropall(c) Flagand control(d) Dropsuperﬂuous(e) WeightedregressionScenario 1Variant i: “unconstrained”1 sextuplet1 octuplet1 decupletVariant ii: “typical”1 sextuplet1 octuplet1 decupletVariant iii: “deviant”1 sextuplet1 octuplet1 decupletVariant iv: “deviant”1 sextuplet1 octuplet1 decupletScenario 2Variant i: “unconstrained”16 doublets40 doublets79 doubletsVariant ii: “typical”16 doublets40 doublets79 doubletsVariant iii: “deviant”16 doublets40 doublets79 doubletsVariant iv: “deviant”16 doublets40 doublets79 doubletsScenario 3Variant i: “unconstrained”7 quintuplets16 quintuplets31 quintupletsVariant ii: “typical”7 quintuplets16 quintuplets31 quintupletsVariant iii: “deviant”7 quintuplets16 quintuplets31 quintupletsVariant iv: “deviant”7 quintuplets16 quintuplets31 quintuplets5:85:85:85:75:75:75:85:85:85:85:85:85:85:75:85:75:75:65:85:96:45:85:96:45:75:75:85:75:75:65:96:27:15:96:17:15:85:85:85:85:85:85:85:85:85:85:85:85:85:96:15:86:06:25:86:06:45:86:06:55:85:96:15:85:96:15:85:96:15:85:96:15:85:85:85:85:85:85:85:85:85:85:85:85:85:85:85:75:75:65:75:65:55:75:96:85:85:85:85:75:75:65:75:75:65:75:85:95:85:85:85:85:85:85:85:85:85:85:85:85:85:85:95:85:85:95:85:85:95:85:85:95:85:96:05:85:96:05:85:96:05:85:96:06:06:06:06:06:06:06:06:06:06:06:06:06:06:06:06:06:06:16:06:06:06:06:06:06:06:16:26:06:16:26:06:16:16:06:16:1